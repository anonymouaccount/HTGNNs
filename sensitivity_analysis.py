# -*- coding: utf-8 -*-
"""sensitivity_analysis.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1DCwbM_lAgYuXuEnvAtGB05pccx2YUBtS
"""

!pip install torch-geometric

"""# **parametre** Last.fm"""

!pip install torch-geometric

!pip install torch-scatter -f https://data.pyg.org/whl/torch-2.6.0+cpu.html

!pip install torch-sparse -f https://data.pyg.org/whl/torch-2.6.0+cpu.html
!pip install torch-geometric

from torch_scatter import scatter_add
print("Successfully installed torch-scatter!")

import pandas as pd
import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch_geometric.data import Data
from torch_geometric.nn import GCNConv
from torch_scatter import scatter_add
from sklearn.model_selection import train_test_split
import networkx as nx
from sklearn.metrics import ndcg_score
from collections import defaultdict
import warnings
warnings.filterwarnings('ignore')

# Step 1: Mount Google Drive
from google.colab import drive
drive.mount('/content/drive')

# Step 2: Unzip the Last.fm.zip file
import zipfile
zip_path = '/content/drive/MyDrive/Last.FM.zip'
with zipfile.ZipFile(zip_path, 'r') as zip_ref:
    zip_ref.extractall('/content/lastfm_data')

# Step 3: Load and preprocess the data
def load_lastfm_data(path):
    df = pd.read_csv(path)
    df['timestamp'] = pd.to_datetime(df['Date'] + ' ' + df['Time'])
    df.rename(columns={'Username': 'user_id', 'Track': 'item_id'}, inplace=True)
    df.dropna(subset=['user_id', 'item_id', 'timestamp'], inplace=True)

    # Filter top active users
    top_users = df['user_id'].value_counts().nlargest(1000).index
    df = df[df['user_id'].isin(top_users)]
    return df.sort_values('timestamp')

csv_path = '/content/lastfm_data/Last.fm_data.csv'
df = load_lastfm_data(csv_path)

# --- Temporal Graph Construction ---
def create_temporal_graph(data, time_granularity='D'):
    """Create graph with temporal edges"""
    data = data.copy()
    data['time_group'] = data['timestamp'].dt.floor(time_granularity)
    G = nx.DiGraph()

    # Add nodes
    for node in set(data['user_id']).union(set(data['item_id'])):
        G.add_node(node)

    # Add temporal edges
    for _, row in data.iterrows():
        G.add_edge(row['user_id'], row['item_id'],
                  timestamp=row['timestamp'].timestamp(),
                  time_group=row['time_group'])
    return G

# --- PyG Data Conversion ---
def graph_to_pyg_data(graph, num_features=32):
    nodes = list(graph.nodes())
    node_map = {n:i for i,n in enumerate(nodes)}

    # Edge features
    edge_index = []
    edge_time = []
    for u,v,data in graph.edges(data=True):
        edge_index.append([node_map[u], node_map[v]])
        edge_time.append(data['timestamp'])

    # Node features
    x = torch.randn(len(nodes), num_features)

    return Data(
        x=x,
        edge_index=torch.tensor(edge_index).t().contiguous(),
        edge_time=torch.tensor(edge_time, dtype=torch.float)
    )

# --- HTGNN Model ---
class HTGNN(nn.Module):
    def __init__(self, in_dim, out_dim, time_dim=16, mem_dim=32):
        super().__init__()
        self.time_emb = nn.Embedding(365, time_dim)
        self.conv1 = GCNConv(in_dim + time_dim, mem_dim)
        self.conv2 = GCNConv(mem_dim, out_dim)

    def forward(self, data):
        # Time embeddings
        time_embeds = self.time_emb((data.edge_time % 365).long())

        # Message passing with time
        x = torch.cat([data.x, scatter_add(time_embeds, data.edge_index[0])], dim=1)
        x = F.relu(self.conv1(x, data.edge_index))
        return self.conv2(x, data.edge_index)

# --- Evaluation Metrics ---
def calculate_ndcg(model, graph, test_interactions, k=10):
    model.eval()
    user_items = defaultdict(set)
    item_users = defaultdict(set)

    # Build interaction dictionaries
    for u,v in graph.edges():
        user_items[u].add(v)
        item_users[v].add(u)

    ndcgs = []
    for user in test_interactions['user_id'].unique():
        # Get test items for user
        pos_items = set(test_interactions[test_interactions['user_id']==user]['item_id'])

        # Sample negatives (items user hasn't interacted with)
        neg_items = set(graph.nodes()) - user_items[user]
        neg_samples = list(neg_items)[:min(100, len(neg_items))]

        # Create test pairs
        test_pairs = [(user,item) for item in list(pos_items)+neg_samples]
        labels = [1 if item in pos_items else 0 for _,item in test_pairs]

        # Get predictions (simplified - replace with real model predictions)
        preds = np.random.rand(len(test_pairs))

        ndcgs.append(ndcg_score([labels], [preds], k=k))

    return np.mean(ndcgs)

# --- Temporal Sensitivity Analysis ---
def temporal_sensitivity_analysis(data, granularities=['D','h','min']):
    results = []

    for gran in granularities:
        print(f"\nAnalyzing granularity: {gran}")

        # Create temporal graph
        graph = create_temporal_graph(data, gran)

        # Temporal train-test split (80% early, 20% late)
        time_groups = sorted({data['time_group'] for _,_,data in graph.edges(data=True)})
        split_idx = int(0.8 * len(time_groups))
        train_edges = [(u,v) for u,v,d in graph.edges(data=True) if d['time_group'] <= time_groups[split_idx]]
        test_edges = [(u,v) for u,v,d in graph.edges(data=True) if d['time_group'] > time_groups[split_idx]]

        # Convert to PyG data
        train_data = graph_to_pyg_data(graph)

        # Train model (placeholder - replace with actual training)
        model = HTGNN(train_data.num_features, 16)

        # Evaluate
        test_df = pd.DataFrame(test_edges, columns=['user_id','item_id'])
        ndcg = calculate_ndcg(model, graph, test_df)

        results.append({
            'Granularity': gran,
            'NDCG@10': ndcg,
            'Train_Edges': len(train_edges),
            'Test_Edges': len(test_edges)
        })

    return pd.DataFrame(results)

# --- Main Execution ---
if __name__ == "__main__":
    # Run sensitivity analysis
    results = temporal_sensitivity_analysis(df)
    print("\nTemporal Sensitivity Results:")
    print(results[['Granularity', 'NDCG@10', 'Train_Edges', 'Test_Edges']])

"""# **parametre** MovieLens"""

# ------------------- Imports -------------------
import pandas as pd
import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch_geometric.data import Data
from torch_geometric.nn import GCNConv
from torch_scatter import scatter_add
from sklearn.model_selection import train_test_split
import networkx as nx
from sklearn.metrics import ndcg_score
from collections import defaultdict
import warnings
warnings.filterwarnings('ignore')

# ------------------- Mount Google Drive -------------------
from google.colab import drive
drive.mount('/content/drive')

# ------------------- Load and preprocess MovieLens data -------------------
def load_movielens_data(path):
    df = pd.read_csv(path)
    df['timestamp'] = pd.to_datetime(df['timestamp'], unit='s')
    df.rename(columns={'userId': 'user_id', 'movieId': 'item_id'}, inplace=True)

    # Optional: keep top users
    top_users = df['user_id'].value_counts().nlargest(1000).index
    df = df[df['user_id'].isin(top_users)]

    return df.sort_values('timestamp')

csv_path = '/content/drive/MyDrive/movielens/ratings_small.csv'
df = load_movielens_data(csv_path)

# ------------------- Temporal Graph Construction -------------------
def create_temporal_graph(data, time_granularity='D'):
    data = data.copy()
    data['time_group'] = data['timestamp'].dt.floor(time_granularity)
    G = nx.DiGraph()
    for node in set(data['user_id']).union(set(data['item_id'])):
        G.add_node(node)
    for _, row in data.iterrows():
        G.add_edge(row['user_id'], row['item_id'],
                   timestamp=row['timestamp'].timestamp(),
                   time_group=row['time_group'])
    return G

# ------------------- PyG Data Conversion -------------------
def graph_to_pyg_data(graph, num_features=32):
    nodes = list(graph.nodes())
    node_map = {n: i for i, n in enumerate(nodes)}
    edge_index = []
    edge_time = []
    for u, v, data in graph.edges(data=True):
        edge_index.append([node_map[u], node_map[v]])
        edge_time.append(data['timestamp'])
    x = torch.randn(len(nodes), num_features)
    return Data(
        x=x,
        edge_index=torch.tensor(edge_index).t().contiguous(),
        edge_time=torch.tensor(edge_time, dtype=torch.float)
    )

# ------------------- HTGNN Model -------------------
class HTGNN(nn.Module):
    def __init__(self, in_dim, out_dim, time_dim=16, mem_dim=32):
        super().__init__()
        self.time_emb = nn.Embedding(365, time_dim)
        self.conv1 = GCNConv(in_dim + time_dim, mem_dim)
        self.conv2 = GCNConv(mem_dim, out_dim)

    def forward(self, data):
        time_embeds = self.time_emb((data.edge_time % 365).long())
        x = torch.cat([data.x, scatter_add(time_embeds, data.edge_index[0], dim=0, dim_size=data.x.size(0))], dim=1)
        x = F.relu(self.conv1(x, data.edge_index))
        return self.conv2(x, data.edge_index)

# ------------------- Evaluation Metrics -------------------
def calculate_ndcg(model, graph, test_interactions, k=10):
    model.eval()
    user_items = defaultdict(set)
    for u, v in graph.edges():
        user_items[u].add(v)
    ndcgs = []
    for user in test_interactions['user_id'].unique():
        pos_items = set(test_interactions[test_interactions['user_id'] == user]['item_id'])
        neg_items = set(graph.nodes()) - user_items[user]
        neg_samples = list(neg_items)[:min(100, len(neg_items))]
        test_pairs = [(user, item) for item in list(pos_items) + neg_samples]
        labels = [1 if item in pos_items else 0 for _, item in test_pairs]
        preds = np.random.rand(len(test_pairs))  # Placeholder
        ndcgs.append(ndcg_score([labels], [preds], k=k))
    return np.mean(ndcgs)

# ------------------- Temporal Sensitivity Analysis -------------------
def temporal_sensitivity_analysis(data, granularities=['D', 'h', 'min']):
    results = []
    for gran in granularities:
        print(f"\nAnalyzing granularity: {gran}")
        graph = create_temporal_graph(data, gran)
        time_groups = sorted({d['time_group'] for _, _, d in graph.edges(data=True)})
        split_idx = int(0.8 * len(time_groups))
        train_edges = [(u, v) for u, v, d in graph.edges(data=True) if d['time_group'] <= time_groups[split_idx]]
        test_edges = [(u, v) for u, v, d in graph.edges(data=True) if d['time_group'] > time_groups[split_idx]]
        train_data = graph_to_pyg_data(graph)
        model = HTGNN(train_data.num_features, 16)
        test_df = pd.DataFrame(test_edges, columns=['user_id', 'item_id'])
        ndcg = calculate_ndcg(model, graph, test_df)
        results.append({
            'Granularity': gran,
            'NDCG@10': ndcg,
            'Train_Edges': len(train_edges),
            'Test_Edges': len(test_edges)
        })
    return pd.DataFrame(results)

# ------------------- Main Execution -------------------
if __name__ == "__main__":
    results = temporal_sensitivity_analysis(df)
    print("\nTemporal Sensitivity Results:")
    print(results[['Granularity', 'NDCG@10', 'Train_Edges', 'Test_Edges']])