{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMlORA50BP5paWLv7x7Ufdy",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anonymouaccount/HTGNNs/blob/main/HTGNN_Experimental_Results.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch torch-geometric numpy matplotlib scikit-learn"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T8ItFvsT89ZM",
        "outputId": "07a7e25b-6943-4220-87f9-9596aded2192"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Collecting torch-geometric\n",
            "  Downloading torch_geometric-2.6.1-py3-none-any.whl.metadata (63 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/63.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.1/63.1 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.0.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (3.11.14)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (5.9.5)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (3.2.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (4.67.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.56.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (11.1.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.14.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (6.2.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (0.3.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (1.18.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric) (2025.1.31)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m71.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m20.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m42.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m16.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m76.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torch_geometric-2.6.1-py3-none-any.whl (1.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m60.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, torch-geometric, nvidia-cusolver-cu12\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 torch-geometric-2.6.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch-geometric"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TskwKfkcqe_m",
        "outputId": "3bdd0060-cd6c-4120-c930-ba9fd4bbcf01"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torch-geometric\n",
            "  Downloading torch_geometric-2.6.1-py3-none-any.whl.metadata (63 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/63.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.1/63.1 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (3.11.13)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (2024.10.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (3.1.6)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (1.26.4)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (5.9.5)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (3.2.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (4.67.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (25.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (0.3.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (1.18.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch-geometric) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric) (2025.1.31)\n",
            "Downloading torch_geometric-2.6.1-py3-none-any.whl (1.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m17.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: torch-geometric\n",
            "Successfully installed torch-geometric-2.6.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **HTGNN_houses**"
      ],
      "metadata": {
        "id": "JNZHrTsFqH6m"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0g4UywDep4X0",
        "outputId": "5f7837e2-c4f4-4d1e-b55c-3110d8723de0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
            "  warnings.warn(out)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: 8.9408, Test Accuracy: 2311.0000\n",
            "Epoch 2, Loss: 7.3028, Test Accuracy: 2305.0000\n",
            "Epoch 3, Loss: 6.2018, Test Accuracy: 2294.0000\n",
            "Epoch 4, Loss: 5.8263, Test Accuracy: 2252.0000\n",
            "Epoch 5, Loss: 5.8350, Test Accuracy: 2250.0000\n",
            "Epoch 6, Loss: 5.7554, Test Accuracy: 2253.0000\n",
            "Epoch 7, Loss: 5.5001, Test Accuracy: 2253.0000\n",
            "Epoch 8, Loss: 5.1183, Test Accuracy: 2241.0000\n",
            "Epoch 9, Loss: 4.6827, Test Accuracy: 2246.0000\n",
            "Epoch 10, Loss: 4.2610, Test Accuracy: 2254.0000\n",
            "NDCG: 0.7179, Precision: 0.4863, Recall: 0.4865, F1-Score: 0.4845\n"
          ]
        }
      ],
      "source": [
        "# Mounting Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Import necessary libraries\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import networkx as nx\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch_geometric.nn import GCNConv\n",
        "from torch_geometric.data import Data, DataLoader\n",
        "import torch.optim as optim\n",
        "\n",
        "# Set environment variables\n",
        "os.environ['LC_ALL'] = 'en_US.UTF-8'\n",
        "\n",
        "# Load the dataset\n",
        "ratings = pd.read_csv('/content/drive/MyDrive/user_activity.csv')\n",
        "\n",
        "# Convert timestamp to datetime and sort by timestamp\n",
        "ratings['timestamp'] = pd.to_datetime(ratings['create_timestamp'])\n",
        "ratings = ratings.sort_values(by='timestamp')\n",
        "\n",
        "# Split the data into train and test sets\n",
        "train_data, test_data = train_test_split(ratings, test_size=0.2, shuffle=False)\n",
        "\n",
        "# Function to create graph from data\n",
        "def create_graph(data):\n",
        "    G = nx.DiGraph()\n",
        "    for _, row in data.iterrows():\n",
        "        # Use the timestamp() method to get a numeric timestamp\n",
        "        G.add_edge(row['user_id'], row['item_id'], timestamp=row['timestamp'].timestamp())\n",
        "    return G\n",
        "\n",
        "train_graph = create_graph(train_data)\n",
        "test_graph = create_graph(test_data)\n",
        "\n",
        "# Function to convert NetworkX graph to PyTorch Geometric Data object\n",
        "def convert_to_pyg_data(graph, num_features=8):\n",
        "    nodes = list(graph.nodes())\n",
        "    node_mapping = {node: i for i, node in enumerate(nodes)}\n",
        "    edge_index = torch.tensor([[node_mapping[u], node_mapping[v]] for u, v in graph.edges]).t().contiguous()\n",
        "    edge_time = torch.tensor([graph[u][v]['timestamp'] for u, v in graph.edges], dtype=torch.float)\n",
        "\n",
        "    # Create a feature matrix with fixed number of features per node\n",
        "    x = torch.randn(len(nodes), num_features)\n",
        "\n",
        "    # Random labels for the nodes (binary classification: 0 or 1)\n",
        "    y = torch.randint(0, 2, (len(nodes),))\n",
        "\n",
        "    data = Data(x=x, edge_index=edge_index, edge_time=edge_time, y=y)\n",
        "    return data\n",
        "\n",
        "train_data_pyg = convert_to_pyg_data(train_graph)\n",
        "test_data_pyg = convert_to_pyg_data(test_graph)\n",
        "\n",
        "train_loader = DataLoader([train_data_pyg], batch_size=1, shuffle=True)\n",
        "test_loader = DataLoader([test_data_pyg], batch_size=1, shuffle=False)\n",
        "\n",
        "# HTGNN Model definition\n",
        "class HTGNN(torch.nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super(HTGNN, self).__init__()\n",
        "        self.conv1 = GCNConv(in_channels, 8)\n",
        "        self.conv2 = GCNConv(8 + 8, out_channels)\n",
        "        self.time_embedding = torch.nn.Embedding(365, 8)  # Embedding for time\n",
        "\n",
        "    def forward(self, x, edge_index, edge_time):\n",
        "        x = self.conv1(x, edge_index)\n",
        "        x = F.relu(x)\n",
        "\n",
        "        # Embedding for edge times\n",
        "        time_embeds = self.time_embedding((edge_time.long() % 365).view(-1, 1)).view(-1, 8)\n",
        "\n",
        "        # Average the edge time embeddings per node\n",
        "        node_time_embeds = torch.zeros_like(x)\n",
        "        for i in range(edge_index.size(1)):\n",
        "            node_time_embeds[edge_index[0, i]] += time_embeds[i]\n",
        "\n",
        "        x = torch.cat([x, node_time_embeds], dim=1)\n",
        "        x = self.conv2(x, edge_index)\n",
        "        return x\n",
        "\n",
        "# Initialize the model, loss function, and optimizer\n",
        "model = HTGNN(in_channels=train_data_pyg.num_node_features, out_channels=2)\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
        "loss_fn = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "# Training function\n",
        "def train(model, loader, optimizer, loss_fn):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for data in loader:\n",
        "        optimizer.zero_grad()\n",
        "        out = model(data.x, data.edge_index, data.edge_time)\n",
        "        loss = loss_fn(out, data.y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "    return total_loss / len(loader)\n",
        "\n",
        "# Evaluation function\n",
        "def evaluate(model, loader):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    for data in loader:\n",
        "        out = model(data.x, data.edge_index, data.edge_time)\n",
        "        pred = out.argmax(dim=1)\n",
        "        correct += (pred == data.y).sum().item()\n",
        "    return correct / len(loader.dataset)\n",
        "\n",
        "# Training loop (now running for only 10 epochs)\n",
        "for epoch in range(10):  # Update: Loop only for 10 epochs\n",
        "    train_loss = train(model, train_loader, optimizer, loss_fn)\n",
        "    test_acc = evaluate(model, test_loader)\n",
        "    print(f'Epoch {epoch+1}, Loss: {train_loss:.4f}, Test Accuracy: {test_acc:.4f}')\n",
        "\n",
        "# Additional evaluation metrics\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
        "\n",
        "# Function to calculate MRR\n",
        "def mrr_score(y_true, y_pred):\n",
        "    order = np.argsort(y_pred)[::-1]\n",
        "    ranks = np.where(y_true[order] == 1)[0] + 1\n",
        "    return np.mean(1.0 / ranks)\n",
        "\n",
        "# Function to calculate NDCG\n",
        "def ndcg_score(y_true, y_pred, k=10):\n",
        "    order = np.argsort(y_pred)[::-1]\n",
        "    y_true = np.take(y_true, order[:k])\n",
        "\n",
        "    gains = 2 ** y_true - 1\n",
        "    discounts = np.log2(np.arange(2, k + 2))\n",
        "    dcg = np.sum(gains / discounts)\n",
        "\n",
        "    ideal_gains = 2 ** np.sort(y_true)[::-1] - 1\n",
        "    idcg = np.sum(ideal_gains / discounts)\n",
        "\n",
        "    return dcg / idcg if idcg > 0 else 0.0\n",
        "\n",
        "# Evaluation function with metrics\n",
        "def evaluate_with_metrics(model, loader):\n",
        "    model.eval()\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "    for data in loader:\n",
        "        out = model(data.x, data.edge_index, data.edge_time)\n",
        "        pred = out.argmax(dim=1)\n",
        "        all_preds.append(pred.detach().cpu().numpy())\n",
        "        all_labels.append(data.y.detach().cpu().numpy())\n",
        "\n",
        "    all_preds = np.concatenate(all_preds)\n",
        "    all_labels = np.concatenate(all_labels)\n",
        "\n",
        "    accuracy = accuracy_score(all_labels, all_preds)\n",
        "    precision = precision_score(all_labels, all_preds, average='macro')\n",
        "    recall = recall_score(all_labels, all_preds, average='macro')\n",
        "    f1 = f1_score(all_labels, all_preds, average='macro')\n",
        "    mrr = mrr_score(all_labels, all_preds)\n",
        "    ndcg = ndcg_score(all_labels, all_preds)\n",
        "\n",
        "    return accuracy, precision, recall, f1, mrr, ndcg\n",
        "\n",
        "# Final evaluation\n",
        "accuracy, precision, recall, f1, mrr, ndcg = evaluate_with_metrics(model, test_loader)\n",
        "print(f'NDCG: {ndcg:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}, F1-Score: {f1:.4f}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# HTGNN-**movies**"
      ],
      "metadata": {
        "id": "EPXu_VO0QFSK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Mounting Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Import necessary libraries\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import networkx as nx\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch_geometric.nn import GCNConv\n",
        "from torch_geometric.data import Data, DataLoader\n",
        "import torch.optim as optim\n",
        "\n",
        "# Set environment variables\n",
        "os.environ['LC_ALL'] = 'en_US.UTF-8'\n",
        "\n",
        "# Load the dataset\n",
        "ratings = pd.read_csv('/content/drive/MyDrive/movielens/ratings_small.csv')\n",
        "\n",
        "# Convert timestamp to datetime and sort by timestamp\n",
        "ratings['timestamp'] = pd.to_datetime(ratings['timestamp'], unit='s')\n",
        "ratings = ratings.sort_values(by='timestamp')\n",
        "\n",
        "# Split the data into train and test sets\n",
        "train_data, test_data = train_test_split(ratings, test_size=0.2, shuffle=False)\n",
        "\n",
        "# Function to create graph from data\n",
        "def create_graph(data):\n",
        "    G = nx.DiGraph()\n",
        "    for _, row in data.iterrows():\n",
        "        G.add_edge(row['userId'], row['movieId'], timestamp=row['timestamp'].timestamp())\n",
        "    return G\n",
        "\n",
        "train_graph = create_graph(train_data)\n",
        "test_graph = create_graph(test_data)\n",
        "\n",
        "# Function to convert NetworkX graph to PyTorch Geometric Data object\n",
        "def convert_to_pyg_data(graph, num_features=8):\n",
        "    nodes = list(graph.nodes())\n",
        "    node_mapping = {node: i for i, node in enumerate(nodes)}\n",
        "    edge_index = torch.tensor([[node_mapping[u], node_mapping[v]] for u, v in graph.edges]).t().contiguous()\n",
        "    edge_time = torch.tensor([graph[u][v]['timestamp'] for u, v in graph.edges], dtype=torch.float)\n",
        "\n",
        "    # Create a feature matrix with fixed number of features per node\n",
        "    x = torch.randn(len(nodes), num_features)\n",
        "\n",
        "    # Random labels for the nodes (binary classification: 0 or 1)\n",
        "    y = torch.randint(0, 2, (len(nodes),))\n",
        "\n",
        "    data = Data(x=x, edge_index=edge_index, edge_time=edge_time, y=y)\n",
        "    return data\n",
        "\n",
        "train_data_pyg = convert_to_pyg_data(train_graph)\n",
        "test_data_pyg = convert_to_pyg_data(test_graph)\n",
        "\n",
        "train_loader = DataLoader([train_data_pyg], batch_size=1, shuffle=True)\n",
        "test_loader = DataLoader([test_data_pyg], batch_size=1, shuffle=False)\n",
        "\n",
        "# HTGNN Model definition\n",
        "class HTGNN(torch.nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super(HTGNN, self).__init__()\n",
        "        self.conv1 = GCNConv(in_channels, 8)\n",
        "        self.conv2 = GCNConv(8 + 8, out_channels)\n",
        "        self.time_embedding = torch.nn.Embedding(365, 8)  # Embedding for time\n",
        "\n",
        "    def forward(self, x, edge_index, edge_time):\n",
        "        x = self.conv1(x, edge_index)\n",
        "        x = F.relu(x)\n",
        "\n",
        "        # Embedding for edge times\n",
        "        time_embeds = self.time_embedding((edge_time.long() % 365).view(-1, 1)).view(-1, 8)\n",
        "\n",
        "        # Average the edge time embeddings per node\n",
        "        node_time_embeds = torch.zeros_like(x)\n",
        "        for i in range(edge_index.size(1)):\n",
        "            node_time_embeds[edge_index[0, i]] += time_embeds[i]\n",
        "\n",
        "        x = torch.cat([x, node_time_embeds], dim=1)\n",
        "        x = self.conv2(x, edge_index)\n",
        "        return x\n",
        "\n",
        "# Initialize the model, loss function, and optimizer\n",
        "model = HTGNN(in_channels=train_data_pyg.num_node_features, out_channels=2)\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
        "loss_fn = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "# Training function\n",
        "def train(model, loader, optimizer, loss_fn):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for data in loader:\n",
        "        optimizer.zero_grad()\n",
        "        out = model(data.x, data.edge_index, data.edge_time)\n",
        "        loss = loss_fn(out, data.y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "    return total_loss / len(loader)\n",
        "\n",
        "# Evaluation function\n",
        "def evaluate(model, loader):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    for data in loader:\n",
        "        out = model(data.x, data.edge_index, data.edge_time)\n",
        "        pred = out.argmax(dim=1)\n",
        "        correct += (pred == data.y).sum().item()\n",
        "    return correct / len(loader.dataset)\n",
        "\n",
        "# Training loop (now running for only 10 epochs)\n",
        "for epoch in range(10):  # Update: Loop only for 10 epochs\n",
        "    train_loss = train(model, train_loader, optimizer, loss_fn)\n",
        "    test_acc = evaluate(model, test_loader)\n",
        "    print(f'Epoch {epoch+1}, Loss: {train_loss:.4f}, Test Accuracy: {test_acc:.4f}')\n",
        "\n",
        "# Additional evaluation metrics\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
        "\n",
        "# Function to calculate MRR\n",
        "def mrr_score(y_true, y_pred):\n",
        "    order = np.argsort(y_pred)[::-1]\n",
        "    ranks = np.where(y_true[order] == 1)[0] + 1\n",
        "    return np.mean(1.0 / ranks)\n",
        "\n",
        "# Function to calculate NDCG\n",
        "def ndcg_score(y_true, y_pred, k=10):\n",
        "    order = np.argsort(y_pred)[::-1]\n",
        "    y_true = np.take(y_true, order[:k])\n",
        "\n",
        "    gains = 2 ** y_true - 1\n",
        "    discounts = np.log2(np.arange(2, k + 2))\n",
        "    dcg = np.sum(gains / discounts)\n",
        "\n",
        "    ideal_gains = 2 ** np.sort(y_true)[::-1] - 1\n",
        "    idcg = np.sum(ideal_gains / discounts)\n",
        "\n",
        "    return dcg / idcg if idcg > 0 else 0.0\n",
        "\n",
        "# Evaluation function with metrics\n",
        "def evaluate_with_metrics(model, loader):\n",
        "    model.eval()\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "    for data in loader:\n",
        "        out = model(data.x, data.edge_index, data.edge_time)\n",
        "        pred = out.argmax(dim=1)\n",
        "        all_preds.append(pred.detach().cpu().numpy())\n",
        "        all_labels.append(data.y.detach().cpu().numpy())\n",
        "\n",
        "    all_preds = np.concatenate(all_preds)\n",
        "    all_labels = np.concatenate(all_labels)\n",
        "\n",
        "    accuracy = accuracy_score(all_labels, all_preds)\n",
        "    precision = precision_score(all_labels, all_preds, average='macro')\n",
        "    recall = recall_score(all_labels, all_preds, average='macro')\n",
        "    f1 = f1_score(all_labels, all_preds, average='macro')\n",
        "    mrr = mrr_score(all_labels, all_preds)\n",
        "    ndcg = ndcg_score(all_labels, all_preds)\n",
        "\n",
        "    return accuracy, precision, recall, f1, mrr, ndcg\n",
        "\n",
        "# Final evaluation\n",
        "accuracy, precision, recall, f1, mrr, ndcg = evaluate_with_metrics(model, test_loader)\n",
        "print(f'NDCG: {ndcg:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}, F1-Score: {f1:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Irp58YQnQJZ_",
        "outputId": "d924d69a-d193-49e9-bfa1-19d67a3b0852"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
            "  warnings.warn(out)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: 15.0247, Test Accuracy: 2429.0000\n",
            "Epoch 2, Loss: 12.1960, Test Accuracy: 2442.0000\n",
            "Epoch 3, Loss: 12.8334, Test Accuracy: 2434.0000\n",
            "Epoch 4, Loss: 11.3649, Test Accuracy: 2420.0000\n",
            "Epoch 5, Loss: 9.9414, Test Accuracy: 2413.0000\n",
            "Epoch 6, Loss: 9.9673, Test Accuracy: 2419.0000\n",
            "Epoch 7, Loss: 9.7144, Test Accuracy: 2414.0000\n",
            "Epoch 8, Loss: 8.7053, Test Accuracy: 2422.0000\n",
            "Epoch 9, Loss: 7.8673, Test Accuracy: 2428.0000\n",
            "Epoch 10, Loss: 7.6751, Test Accuracy: 2429.0000\n",
            "NDCG: 0.9748, Precision: 0.5007, Recall: 0.5007, F1-Score: 0.4987\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# HTGNN-**amazon**"
      ],
      "metadata": {
        "id": "NeUALTreT7DM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Mounting Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Import necessary libraries\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import networkx as nx\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch_geometric.nn import GCNConv\n",
        "from torch_geometric.data import Data, DataLoader\n",
        "import torch.optim as optim\n",
        "\n",
        "# Set environment variables\n",
        "os.environ['LC_ALL'] = 'en_US.UTF-8'\n",
        "\n",
        "# Load the dataset\n",
        "ratings = pd.read_csv('/content/drive/MyDrive/ratings_Beauty.csv')\n",
        "\n",
        "# Display the first few rows to check the structure of the dataset\n",
        "print(ratings.head())\n",
        "\n",
        "ratings['timestamp'] = pd.to_datetime(ratings['Timestamp'], unit='s')\n",
        "ratings = ratings.sort_values(by='timestamp')\n",
        "\n",
        "# Split the data into train and test sets\n",
        "train_data, test_data = train_test_split(ratings, test_size=0.2, shuffle=False)\n",
        "\n",
        "train_data = train_data.sample(frac=0.1, random_state=42)\n",
        "test_data = test_data.sample(frac=0.1, random_state=42)\n",
        "\n",
        "# Function to create graph from data\n",
        "def create_graph(data):\n",
        "    G = nx.DiGraph()\n",
        "    for _, row in data.iterrows():\n",
        "        G.add_edge(row['UserId'], row['ProductId'], timestamp=row['Timestamp'])\n",
        "    return G\n",
        "\n",
        "train_graph = create_graph(train_data)\n",
        "test_graph = create_graph(test_data)\n",
        "\n",
        "# Function to convert NetworkX graph to PyTorch Geometric Data object\n",
        "def convert_to_pyg_data(graph, num_features=8):\n",
        "    nodes = list(graph.nodes())\n",
        "    node_mapping = {node: i for i, node in enumerate(nodes)}\n",
        "    edge_index = torch.tensor([[node_mapping[u], node_mapping[v]] for u, v in graph.edges]).t().contiguous()\n",
        "    edge_time = torch.tensor([graph[u][v]['timestamp'] for u, v in graph.edges], dtype=torch.float)\n",
        "\n",
        "    # Create a feature matrix with reduced number of features per node (e.g., 4 features instead of 8)\n",
        "    x = torch.randn(len(nodes), num_features)\n",
        "\n",
        "    # Random labels for the nodes (binary classification: 0 or 1)\n",
        "    y = torch.randint(0, 2, (len(nodes),))\n",
        "\n",
        "    data = Data(x=x, edge_index=edge_index, edge_time=edge_time, y=y)\n",
        "    return data\n",
        "\n",
        "# Convert graph data to PyTorch Geometric Data format\n",
        "train_data_pyg = convert_to_pyg_data(train_graph, num_features=4)  # Reduced features\n",
        "test_data_pyg = convert_to_pyg_data(test_graph, num_features=4)  # Reduced features\n",
        "\n",
        "# Create data loaders with smaller batch size\n",
        "train_loader = DataLoader([train_data_pyg], batch_size=1, shuffle=True)  # Batch size 1\n",
        "test_loader = DataLoader([test_data_pyg], batch_size=1, shuffle=False)  # Batch size 1\n",
        "\n",
        "# HTGNN Model definition\n",
        "class HTGNN(torch.nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super(HTGNN, self).__init__()\n",
        "        self.conv1 = GCNConv(in_channels, 8)\n",
        "        self.conv2 = GCNConv(8 + 8, out_channels)\n",
        "        self.time_embedding = torch.nn.Embedding(365, 8)  # Embedding for time\n",
        "\n",
        "    def forward(self, x, edge_index, edge_time):\n",
        "        x = self.conv1(x, edge_index)\n",
        "        x = F.relu(x)\n",
        "\n",
        "        # Embedding for edge times\n",
        "        time_embeds = self.time_embedding((edge_time.long() % 365).view(-1, 1)).view(-1, 8)\n",
        "\n",
        "        # Average the edge time embeddings per node\n",
        "        node_time_embeds = torch.zeros_like(x)\n",
        "        for i in range(edge_index.size(1)):\n",
        "            node_time_embeds[edge_index[0, i]] += time_embeds[i]\n",
        "\n",
        "        x = torch.cat([x, node_time_embeds], dim=1)\n",
        "        x = self.conv2(x, edge_index)\n",
        "        return x\n",
        "\n",
        "# Initialize the model, loss function, and optimizer\n",
        "model = HTGNN(in_channels=train_data_pyg.num_node_features, out_channels=2)\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
        "loss_fn = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "# Training function\n",
        "def train(model, loader, optimizer, loss_fn):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for data in loader:\n",
        "        optimizer.zero_grad()\n",
        "        out = model(data.x, data.edge_index, data.edge_time)\n",
        "        loss = loss_fn(out, data.y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "    return total_loss / len(loader)\n",
        "\n",
        "# Evaluation function\n",
        "def evaluate(model, loader):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    for data in loader:\n",
        "        out = model(data.x, data.edge_index, data.edge_time)\n",
        "        pred = out.argmax(dim=1)\n",
        "        correct += (pred == data.y).sum().item()\n",
        "    return correct / len(loader.dataset)\n",
        "\n",
        "# Training loop (now running for only 10 epochs)\n",
        "for epoch in range(10):  # Update: Loop only for 10 epochs\n",
        "    train_loss = train(model, train_loader, optimizer, loss_fn)\n",
        "    test_acc = evaluate(model, test_loader)\n",
        "    print(f'Epoch {epoch+1}, Loss: {train_loss:.4f}, Test Accuracy: {test_acc:.4f}')\n",
        "\n",
        "# Additional evaluation metrics\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
        "\n",
        "# Function to calculate MRR\n",
        "def mrr_score(y_true, y_pred):\n",
        "    order = np.argsort(y_pred)[::-1]\n",
        "    ranks = np.where(y_true[order] == 1)[0] + 1\n",
        "    return np.mean(1.0 / ranks)\n",
        "\n",
        "# Function to calculate NDCG\n",
        "def ndcg_score(y_true, y_pred, k=10):\n",
        "    order = np.argsort(y_pred)[::-1]\n",
        "    y_true = np.take(y_true, order[:k])\n",
        "\n",
        "    gains = 2 ** y_true - 1\n",
        "    discounts = np.log2(np.arange(2, k + 2))\n",
        "    dcg = np.sum(gains / discounts)\n",
        "\n",
        "    ideal_gains = 2 ** np.sort(y_true)[::-1] - 1\n",
        "    idcg = np.sum(ideal_gains / discounts)\n",
        "\n",
        "    return dcg / idcg if idcg > 0 else 0.0\n",
        "\n",
        "# Evaluation function with metrics\n",
        "def evaluate_with_metrics(model, loader):\n",
        "    model.eval()\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "    for data in loader:\n",
        "        out = model(data.x, data.edge_index, data.edge_time)\n",
        "        pred = out.argmax(dim=1)\n",
        "        all_preds.append(pred.detach().cpu().numpy())\n",
        "        all_labels.append(data.y.detach().cpu().numpy())\n",
        "\n",
        "    all_preds = np.concatenate(all_preds)\n",
        "    all_labels = np.concatenate(all_labels)\n",
        "\n",
        "    accuracy = accuracy_score(all_labels, all_preds)\n",
        "    precision = precision_score(all_labels, all_preds, average='macro')\n",
        "    recall = recall_score(all_labels, all_preds, average='macro')\n",
        "    f1 = f1_score(all_labels, all_preds, average='macro')\n",
        "    mrr = mrr_score(all_labels, all_preds)\n",
        "    ndcg = ndcg_score(all_labels, all_preds)\n",
        "\n",
        "    return accuracy, precision, recall, f1, mrr, ndcg\n",
        "\n",
        "# Final evaluation\n",
        "accuracy, precision, recall, f1, mrr, ndcg = evaluate_with_metrics(model, test_loader)\n",
        "print(f'NDCG: {ndcg:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}, F1-Score: {f1:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a8d--4IAT_zG",
        "outputId": "603e579f-291f-4b72-b07a-cc421a98670d"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "           UserId   ProductId  Rating   Timestamp\n",
            "0  A39HTATAQ9V7YF  0205616461     5.0  1369699200\n",
            "1  A3JM6GV9MNOF9X  0558925278     3.0  1355443200\n",
            "2  A1Z513UWSAAO0F  0558925278     5.0  1404691200\n",
            "3  A1WMRR494NWEWV  0733001998     4.0  1382572800\n",
            "4  A3IAAVS479H7M7  0737104473     1.0  1274227200\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
            "  warnings.warn(out)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: 0.9077, Test Accuracy: 30768.0000\n",
            "Epoch 2, Loss: 0.8840, Test Accuracy: 30821.0000\n",
            "Epoch 3, Loss: 0.8630, Test Accuracy: 30778.0000\n",
            "Epoch 4, Loss: 0.8443, Test Accuracy: 30818.0000\n",
            "Epoch 5, Loss: 0.8276, Test Accuracy: 30829.0000\n",
            "Epoch 6, Loss: 0.8127, Test Accuracy: 30815.0000\n",
            "Epoch 7, Loss: 0.7994, Test Accuracy: 30838.0000\n",
            "Epoch 8, Loss: 0.7877, Test Accuracy: 30821.0000\n",
            "Epoch 9, Loss: 0.7773, Test Accuracy: 30802.0000\n",
            "Epoch 10, Loss: 0.7681, Test Accuracy: 30801.0000\n",
            "NDCG: 0.8878, Precision: 0.5014, Recall: 0.5014, F1-Score: 0.5014\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(ratings.columns)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AuVWXedRVDSU",
        "outputId": "f9a0b569-4db5-489d-ef2c-990ad2df74c1"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index(['UserId', 'ProductId', 'Rating', 'Timestamp'], dtype='object')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GraphSAGE-**houses**"
      ],
      "metadata": {
        "id": "N5efLfNL8ClC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Mounting Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Import necessary libraries\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import networkx as nx\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch_geometric.nn import SAGEConv  # Use SAGEConv for GraphSAGE\n",
        "from torch_geometric.data import Data, DataLoader\n",
        "import torch.optim as optim\n",
        "\n",
        "# Set environment variables\n",
        "os.environ['LC_ALL'] = 'en_US.UTF-8'\n",
        "\n",
        "# Load the dataset\n",
        "ratings = pd.read_csv('/content/drive/MyDrive/user_activity.csv')\n",
        "\n",
        "# Convert timestamp to datetime and sort by timestamp\n",
        "ratings['timestamp'] = pd.to_datetime(ratings['create_timestamp'])\n",
        "ratings = ratings.sort_values(by='timestamp')\n",
        "\n",
        "# Split the data into train and test sets\n",
        "train_data, test_data = train_test_split(ratings, test_size=0.2, shuffle=False)\n",
        "\n",
        "# Function to create graph from data\n",
        "def create_graph(data):\n",
        "    G = nx.DiGraph()\n",
        "    for _, row in data.iterrows():\n",
        "        # Use the timestamp() method to get a numeric timestamp\n",
        "        G.add_edge(row['user_id'], row['item_id'], timestamp=row['timestamp'].timestamp())\n",
        "    return G\n",
        "\n",
        "train_graph = create_graph(train_data)\n",
        "test_graph = create_graph(test_data)\n",
        "\n",
        "# Function to convert NetworkX graph to PyTorch Geometric Data object\n",
        "def convert_to_pyg_data(graph, num_features=8):\n",
        "    nodes = list(graph.nodes())\n",
        "    node_mapping = {node: i for i, node in enumerate(nodes)}\n",
        "    edge_index = torch.tensor([[node_mapping[u], node_mapping[v]] for u, v in graph.edges]).t().contiguous()\n",
        "    edge_time = torch.tensor([graph[u][v]['timestamp'] for u, v in graph.edges], dtype=torch.float)\n",
        "\n",
        "    # Create a feature matrix with fixed number of features per node\n",
        "    x = torch.randn(len(nodes), num_features)\n",
        "\n",
        "    # Random labels for the nodes (binary classification: 0 or 1)\n",
        "    y = torch.randint(0, 2, (len(nodes),))\n",
        "\n",
        "    data = Data(x=x, edge_index=edge_index, edge_time=edge_time, y=y)\n",
        "    return data\n",
        "\n",
        "train_data_pyg = convert_to_pyg_data(train_graph)\n",
        "test_data_pyg = convert_to_pyg_data(test_graph)\n",
        "\n",
        "train_loader = DataLoader([train_data_pyg], batch_size=1, shuffle=True)\n",
        "test_loader = DataLoader([test_data_pyg], batch_size=1, shuffle=False)\n",
        "\n",
        "# GraphSAGE Model definition\n",
        "class GraphSAGE(torch.nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super(GraphSAGE, self).__init__()\n",
        "        self.conv1 = SAGEConv(in_channels, 8)  # First GraphSAGE layer\n",
        "        self.conv2 = SAGEConv(8, out_channels)  # Second GraphSAGE layer\n",
        "\n",
        "    def forward(self, x, edge_index):\n",
        "        x = self.conv1(x, edge_index)\n",
        "        x = F.relu(x)\n",
        "        x = self.conv2(x, edge_index)\n",
        "        return x\n",
        "\n",
        "# Initialize the model, loss function, and optimizer\n",
        "model = GraphSAGE(in_channels=train_data_pyg.num_node_features, out_channels=2)\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
        "loss_fn = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "# Training function\n",
        "def train(model, loader, optimizer, loss_fn):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for data in loader:\n",
        "        optimizer.zero_grad()\n",
        "        out = model(data.x, data.edge_index)  # GraphSAGE does not use edge_time\n",
        "        loss = loss_fn(out, data.y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "    return total_loss / len(loader)\n",
        "\n",
        "# Evaluation function\n",
        "def evaluate(model, loader):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    for data in loader:\n",
        "        out = model(data.x, data.edge_index)  # GraphSAGE does not use edge_time\n",
        "        pred = out.argmax(dim=1)\n",
        "        correct += (pred == data.y).sum().item()\n",
        "    return correct / len(loader.dataset)\n",
        "\n",
        "# Training loop (now running for only 10 epochs)\n",
        "for epoch in range(10):  # Update: Loop only for 10 epochs\n",
        "    train_loss = train(model, train_loader, optimizer, loss_fn)\n",
        "    test_acc = evaluate(model, test_loader)\n",
        "    print(f'Epoch {epoch+1}, Loss: {train_loss:.4f}, Test Accuracy: {test_acc:.4f}')\n",
        "\n",
        "# Additional evaluation metrics\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
        "\n",
        "# Function to calculate MRR\n",
        "def mrr_score(y_true, y_pred):\n",
        "    order = np.argsort(y_pred)[::-1]\n",
        "    ranks = np.where(y_true[order] == 1)[0] + 1\n",
        "    return np.mean(1.0 / ranks)\n",
        "\n",
        "# Function to calculate NDCG\n",
        "def ndcg_score(y_true, y_pred, k=10):\n",
        "    order = np.argsort(y_pred)[::-1]\n",
        "    y_true = np.take(y_true, order[:k])\n",
        "\n",
        "    gains = 2 ** y_true - 1\n",
        "    discounts = np.log2(np.arange(2, k + 2))\n",
        "    dcg = np.sum(gains / discounts)\n",
        "\n",
        "    ideal_gains = 2 ** np.sort(y_true)[::-1] - 1\n",
        "    idcg = np.sum(ideal_gains / discounts)\n",
        "\n",
        "    return dcg / idcg if idcg > 0 else 0.0\n",
        "\n",
        "# Evaluation function with metrics\n",
        "def evaluate_with_metrics(model, loader):\n",
        "    model.eval()\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "    for data in loader:\n",
        "        out = model(data.x, data.edge_index)  # GraphSAGE does not use edge_time\n",
        "        pred = out.argmax(dim=1)\n",
        "        all_preds.append(pred.detach().cpu().numpy())\n",
        "        all_labels.append(data.y.detach().cpu().numpy())\n",
        "\n",
        "    all_preds = np.concatenate(all_preds)\n",
        "    all_labels = np.concatenate(all_labels)\n",
        "\n",
        "    accuracy = accuracy_score(all_labels, all_preds)\n",
        "    precision = precision_score(all_labels, all_preds, average='macro')\n",
        "    recall = recall_score(all_labels, all_preds, average='macro')\n",
        "    f1 = f1_score(all_labels, all_preds, average='macro')\n",
        "    mrr = mrr_score(all_labels, all_preds)\n",
        "    ndcg = ndcg_score(all_labels, all_preds)\n",
        "\n",
        "    return accuracy, precision, recall, f1, mrr, ndcg\n",
        "\n",
        "# Final evaluation\n",
        "accuracy, precision, recall, f1, mrr, ndcg = evaluate_with_metrics(model, test_loader)\n",
        "print(f'NDCG: {ndcg:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}, F1-Score: {f1:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aNlEFrly7-4-",
        "outputId": "473d65c0-c76d-44ee-b428-643c253bfe12"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
            "  warnings.warn(out)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: 0.7063, Test Accuracy: 2335.0000\n",
            "Epoch 2, Loss: 0.7010, Test Accuracy: 2308.0000\n",
            "Epoch 3, Loss: 0.6979, Test Accuracy: 2340.0000\n",
            "Epoch 4, Loss: 0.6964, Test Accuracy: 2318.0000\n",
            "Epoch 5, Loss: 0.6958, Test Accuracy: 2313.0000\n",
            "Epoch 6, Loss: 0.6954, Test Accuracy: 2310.0000\n",
            "Epoch 7, Loss: 0.6951, Test Accuracy: 2309.0000\n",
            "Epoch 8, Loss: 0.6947, Test Accuracy: 2348.0000\n",
            "Epoch 9, Loss: 0.6943, Test Accuracy: 2376.0000\n",
            "Epoch 10, Loss: 0.6940, Test Accuracy: 2369.0000\n",
            "NDCG: 0.5641, Precision: 0.5133, Recall: 0.5132, F1-Score: 0.5118\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GraphSAGE-**movies**"
      ],
      "metadata": {
        "id": "qYQ3I-PBSXyH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Mounting Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Import necessary libraries\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import networkx as nx\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch_geometric.nn import SAGEConv  # Use SAGEConv for GraphSAGE\n",
        "from torch_geometric.data import Data, DataLoader\n",
        "import torch.optim as optim\n",
        "\n",
        "# Set environment variables\n",
        "os.environ['LC_ALL'] = 'en_US.UTF-8'\n",
        "\n",
        "# Load the MovieLens dataset\n",
        "ratings = pd.read_csv('/content/drive/MyDrive/movielens/ratings_small.csv')\n",
        "\n",
        "# Convert timestamp to datetime and sort by timestamp\n",
        "ratings['timestamp'] = pd.to_datetime(ratings['timestamp'])\n",
        "ratings = ratings.sort_values(by='timestamp')\n",
        "\n",
        "# Split the data into train and test sets\n",
        "train_data, test_data = train_test_split(ratings, test_size=0.2, shuffle=False)\n",
        "\n",
        "# Function to create graph from data\n",
        "def create_graph(data):\n",
        "    G = nx.DiGraph()\n",
        "    for _, row in data.iterrows():\n",
        "        # Use the timestamp() method to get a numeric timestamp\n",
        "        G.add_edge(row['userId'], row['movieId'], timestamp=row['timestamp'].timestamp())\n",
        "    return G\n",
        "\n",
        "train_graph = create_graph(train_data)\n",
        "test_graph = create_graph(test_data)\n",
        "\n",
        "# Function to convert NetworkX graph to PyTorch Geometric Data object\n",
        "def convert_to_pyg_data(graph, num_features=8):\n",
        "    nodes = list(graph.nodes())\n",
        "    node_mapping = {node: i for i, node in enumerate(nodes)}\n",
        "    edge_index = torch.tensor([[node_mapping[u], node_mapping[v]] for u, v in graph.edges]).t().contiguous()\n",
        "    edge_time = torch.tensor([graph[u][v]['timestamp'] for u, v in graph.edges], dtype=torch.float)\n",
        "\n",
        "    # Create a feature matrix with fixed number of features per node\n",
        "    x = torch.randn(len(nodes), num_features)\n",
        "\n",
        "    # Random labels for the nodes (binary classification: 0 or 1)\n",
        "    y = torch.randint(0, 2, (len(nodes),))\n",
        "\n",
        "    data = Data(x=x, edge_index=edge_index, edge_time=edge_time, y=y)\n",
        "    return data\n",
        "\n",
        "train_data_pyg = convert_to_pyg_data(train_graph)\n",
        "test_data_pyg = convert_to_pyg_data(test_graph)\n",
        "\n",
        "train_loader = DataLoader([train_data_pyg], batch_size=1, shuffle=True)\n",
        "test_loader = DataLoader([test_data_pyg], batch_size=1, shuffle=False)\n",
        "\n",
        "# GraphSAGE Model definition\n",
        "class GraphSAGE(torch.nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super(GraphSAGE, self).__init__()\n",
        "        self.conv1 = SAGEConv(in_channels, 8)  # First GraphSAGE layer\n",
        "        self.conv2 = SAGEConv(8, out_channels)  # Second GraphSAGE layer\n",
        "\n",
        "    def forward(self, x, edge_index):\n",
        "        x = self.conv1(x, edge_index)\n",
        "        x = F.relu(x)\n",
        "        x = self.conv2(x, edge_index)\n",
        "        return x\n",
        "\n",
        "# Initialize the model, loss function, and optimizer\n",
        "model = GraphSAGE(in_channels=train_data_pyg.num_node_features, out_channels=2)\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
        "loss_fn = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "# Training function\n",
        "def train(model, loader, optimizer, loss_fn):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for data in loader:\n",
        "        optimizer.zero_grad()\n",
        "        out = model(data.x, data.edge_index)  # GraphSAGE does not use edge_time\n",
        "        loss = loss_fn(out, data.y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "    return total_loss / len(loader)\n",
        "\n",
        "# Evaluation function\n",
        "def evaluate(model, loader):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    for data in loader:\n",
        "        out = model(data.x, data.edge_index)  # GraphSAGE does not use edge_time\n",
        "        pred = out.argmax(dim=1)\n",
        "        correct += (pred == data.y).sum().item()\n",
        "    return correct / len(loader.dataset)\n",
        "\n",
        "# Training loop (now running for only 10 epochs)\n",
        "for epoch in range(10):  # Update: Loop only for 10 epochs\n",
        "    train_loss = train(model, train_loader, optimizer, loss_fn)\n",
        "    test_acc = evaluate(model, test_loader)\n",
        "    print(f'Epoch {epoch+1}, Loss: {train_loss:.4f}, Test Accuracy: {test_acc:.4f}')\n",
        "\n",
        "# Additional evaluation metrics\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
        "\n",
        "# Function to calculate MRR\n",
        "def mrr_score(y_true, y_pred):\n",
        "    order = np.argsort(y_pred)[::-1]\n",
        "    ranks = np.where(y_true[order] == 1)[0] + 1\n",
        "    return np.mean(1.0 / ranks)\n",
        "\n",
        "# Function to calculate NDCG\n",
        "def ndcg_score(y_true, y_pred, k=10):\n",
        "    order = np.argsort(y_pred)[::-1]\n",
        "    y_true = np.take(y_true, order[:k])\n",
        "\n",
        "    gains = 2 ** y_true - 1\n",
        "    discounts = np.log2(np.arange(2, k + 2))\n",
        "    dcg = np.sum(gains / discounts)\n",
        "\n",
        "    ideal_gains = 2 ** np.sort(y_true)[::-1] - 1\n",
        "    idcg = np.sum(ideal_gains / discounts)\n",
        "\n",
        "    return dcg / idcg if idcg > 0 else 0.0\n",
        "\n",
        "# Evaluation function with metrics\n",
        "def evaluate_with_metrics(model, loader):\n",
        "    model.eval()\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "    for data in loader:\n",
        "        out = model(data.x, data.edge_index)  # GraphSAGE does not use edge_time\n",
        "        pred = out.argmax(dim=1)\n",
        "        all_preds.append(pred.detach().cpu().numpy())\n",
        "        all_labels.append(data.y.detach().cpu().numpy())\n",
        "\n",
        "    all_preds = np.concatenate(all_preds)\n",
        "    all_labels = np.concatenate(all_labels)\n",
        "\n",
        "    accuracy = accuracy_score(all_labels, all_preds)\n",
        "    precision = precision_score(all_labels, all_preds, average='macro')\n",
        "    recall = recall_score(all_labels, all_preds, average='macro')\n",
        "    f1 = f1_score(all_labels, all_preds, average='macro')\n",
        "    mrr = mrr_score(all_labels, all_preds)\n",
        "    ndcg = ndcg_score(all_labels, all_preds)\n",
        "\n",
        "    return accuracy, precision, recall, f1, mrr, ndcg\n",
        "\n",
        "# Final evaluation\n",
        "accuracy, precision, recall, f1, mrr, ndcg = evaluate_with_metrics(model, test_loader)\n",
        "print(f'NDCG: {ndcg:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}, F1-Score: {f1:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lawRuqKKSdgK",
        "outputId": "1a0ef430-821c-45ee-a5cc-a1907df0e5e1"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
            "  warnings.warn(out)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: 0.7851, Test Accuracy: 2420.0000\n",
            "Epoch 2, Loss: 0.7560, Test Accuracy: 2435.0000\n",
            "Epoch 3, Loss: 0.7338, Test Accuracy: 2439.0000\n",
            "Epoch 4, Loss: 0.7183, Test Accuracy: 2421.0000\n",
            "Epoch 5, Loss: 0.7086, Test Accuracy: 2405.0000\n",
            "Epoch 6, Loss: 0.7038, Test Accuracy: 2420.0000\n",
            "Epoch 7, Loss: 0.7025, Test Accuracy: 2426.0000\n",
            "Epoch 8, Loss: 0.7031, Test Accuracy: 2414.0000\n",
            "Epoch 9, Loss: 0.7041, Test Accuracy: 2390.0000\n",
            "Epoch 10, Loss: 0.7046, Test Accuracy: 2390.0000\n",
            "NDCG: 0.5257, Precision: 0.4925, Recall: 0.4928, F1-Score: 0.4878\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GraphSAGE-**amazon**"
      ],
      "metadata": {
        "id": "CuA2p4OHxJIQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Mounting Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Import necessary libraries\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import networkx as nx\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch_geometric.nn import SAGEConv  # Use SAGEConv for GraphSAGE\n",
        "from torch_geometric.data import Data, DataLoader\n",
        "import torch.optim as optim\n",
        "\n",
        "# Set environment variables\n",
        "os.environ['LC_ALL'] = 'en_US.UTF-8'\n",
        "\n",
        "# Load the Beauty dataset\n",
        "ratings = pd.read_csv('/content/drive/MyDrive/ratings_Beauty.csv')\n",
        "\n",
        "# Display the first few rows to check the structure of the dataset\n",
        "print(ratings.head())\n",
        "\n",
        "ratings['timestamp'] = pd.to_datetime(ratings['Timestamp'], unit='s')\n",
        "ratings = ratings.sort_values(by='timestamp')\n",
        "\n",
        "# Split the data into train and test sets\n",
        "train_data, test_data = train_test_split(ratings, test_size=0.2, shuffle=False)\n",
        "\n",
        "train_data = train_data.sample(frac=0.1, random_state=42)\n",
        "test_data = test_data.sample(frac=0.1, random_state=42)\n",
        "\n",
        "# Function to create graph from data\n",
        "def create_graph(data):\n",
        "    G = nx.DiGraph()\n",
        "    for _, row in data.iterrows():\n",
        "        # Use the timestamp() method to get a numeric timestamp\n",
        "        G.add_edge(row['UserId'], row['ProductId'], timestamp=row['Timestamp'])\n",
        "    return G\n",
        "\n",
        "train_graph = create_graph(train_data)\n",
        "test_graph = create_graph(test_data)\n",
        "\n",
        "# Function to convert NetworkX graph to PyTorch Geometric Data object\n",
        "def convert_to_pyg_data(graph, num_features=8):\n",
        "    nodes = list(graph.nodes())\n",
        "    node_mapping = {node: i for i, node in enumerate(nodes)}\n",
        "    edge_index = torch.tensor([[node_mapping[u], node_mapping[v]] for u, v in graph.edges]).t().contiguous()\n",
        "    edge_time = torch.tensor([graph[u][v]['timestamp'] for u, v in graph.edges], dtype=torch.float)\n",
        "\n",
        "    # Create a feature matrix with fixed number of features per node\n",
        "    x = torch.randn(len(nodes), num_features)\n",
        "\n",
        "    # Random labels for the nodes (binary classification: 0 or 1)\n",
        "    y = torch.randint(0, 2, (len(nodes),))\n",
        "\n",
        "    data = Data(x=x, edge_index=edge_index, edge_time=edge_time, y=y)\n",
        "    return data\n",
        "\n",
        "train_data_pyg = convert_to_pyg_data(train_graph)\n",
        "test_data_pyg = convert_to_pyg_data(test_graph)\n",
        "\n",
        "train_loader = DataLoader([train_data_pyg], batch_size=1, shuffle=True)\n",
        "test_loader = DataLoader([test_data_pyg], batch_size=1, shuffle=False)\n",
        "\n",
        "# GraphSAGE Model definition\n",
        "class GraphSAGE(torch.nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super(GraphSAGE, self).__init__()\n",
        "        self.conv1 = SAGEConv(in_channels, 8)  # First GraphSAGE layer\n",
        "        self.conv2 = SAGEConv(8, out_channels)  # Second GraphSAGE layer\n",
        "\n",
        "    def forward(self, x, edge_index):\n",
        "        x = self.conv1(x, edge_index)\n",
        "        x = F.relu(x)\n",
        "        x = self.conv2(x, edge_index)\n",
        "        return x\n",
        "\n",
        "# Initialize the model, loss function, and optimizer\n",
        "model = GraphSAGE(in_channels=train_data_pyg.num_node_features, out_channels=2)\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
        "loss_fn = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "# Training function\n",
        "def train(model, loader, optimizer, loss_fn):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for data in loader:\n",
        "        optimizer.zero_grad()\n",
        "        out = model(data.x, data.edge_index)  # GraphSAGE does not use edge_time\n",
        "        loss = loss_fn(out, data.y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "    return total_loss / len(loader)\n",
        "\n",
        "# Evaluation function\n",
        "def evaluate(model, loader):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    for data in loader:\n",
        "        out = model(data.x, data.edge_index)  # GraphSAGE does not use edge_time\n",
        "        pred = out.argmax(dim=1)\n",
        "        correct += (pred == data.y).sum().item()\n",
        "    return correct / len(loader.dataset)\n",
        "\n",
        "# Training loop (now running for only 10 epochs)\n",
        "for epoch in range(10):  # Update: Loop only for 10 epochs\n",
        "    train_loss = train(model, train_loader, optimizer, loss_fn)\n",
        "    test_acc = evaluate(model, test_loader)\n",
        "    print(f'Epoch {epoch+1}, Loss: {train_loss:.4f}, Test Accuracy: {test_acc:.4f}')\n",
        "\n",
        "# Additional evaluation metrics\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
        "\n",
        "# Function to calculate MRR\n",
        "def mrr_score(y_true, y_pred):\n",
        "    order = np.argsort(y_pred)[::-1]\n",
        "    ranks = np.where(y_true[order] == 1)[0] + 1\n",
        "    return np.mean(1.0 / ranks)\n",
        "\n",
        "# Function to calculate NDCG\n",
        "def ndcg_score(y_true, y_pred, k=10):\n",
        "    order = np.argsort(y_pred)[::-1]\n",
        "    y_true = np.take(y_true, order[:k])\n",
        "\n",
        "    gains = 2 ** y_true - 1\n",
        "    discounts = np.log2(np.arange(2, k + 2))\n",
        "    dcg = np.sum(gains / discounts)\n",
        "\n",
        "    ideal_gains = 2 ** np.sort(y_true)[::-1] - 1\n",
        "    idcg = np.sum(ideal_gains / discounts)\n",
        "\n",
        "    return dcg / idcg if idcg > 0 else 0.0\n",
        "\n",
        "# Evaluation function with metrics\n",
        "def evaluate_with_metrics(model, loader):\n",
        "    model.eval()\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "    for data in loader:\n",
        "        out = model(data.x, data.edge_index)  # GraphSAGE does not use edge_time\n",
        "        pred = out.argmax(dim=1)\n",
        "        all_preds.append(pred.detach().cpu().numpy())\n",
        "        all_labels.append(data.y.detach().cpu().numpy())\n",
        "\n",
        "    all_preds = np.concatenate(all_preds)\n",
        "    all_labels = np.concatenate(all_labels)\n",
        "\n",
        "    accuracy = accuracy_score(all_labels, all_preds)\n",
        "    precision = precision_score(all_labels, all_preds, average='macro')\n",
        "    recall = recall_score(all_labels, all_preds, average='macro')\n",
        "    f1 = f1_score(all_labels, all_preds, average='macro')\n",
        "    mrr = mrr_score(all_labels, all_preds)\n",
        "    ndcg = ndcg_score(all_labels, all_preds)\n",
        "\n",
        "    return accuracy, precision, recall, f1, mrr, ndcg\n",
        "\n",
        "# Final evaluation\n",
        "accuracy, precision, recall, f1, mrr, ndcg = evaluate_with_metrics(model, test_loader)\n",
        "print(f'NDCG: {ndcg:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}, F1-Score: {f1:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tLqCt-sgxOGW",
        "outputId": "a0245300-8b34-4699-d99e-b04a2ecbf870"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "           UserId   ProductId  Rating   Timestamp\n",
            "0  A39HTATAQ9V7YF  0205616461     5.0  1369699200\n",
            "1  A3JM6GV9MNOF9X  0558925278     3.0  1355443200\n",
            "2  A1Z513UWSAAO0F  0558925278     5.0  1404691200\n",
            "3  A1WMRR494NWEWV  0733001998     4.0  1382572800\n",
            "4  A3IAAVS479H7M7  0737104473     1.0  1274227200\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
            "  warnings.warn(out)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: 0.7067, Test Accuracy: 30706.0000\n",
            "Epoch 2, Loss: 0.7008, Test Accuracy: 30732.0000\n",
            "Epoch 3, Loss: 0.6979, Test Accuracy: 30723.0000\n",
            "Epoch 4, Loss: 0.6970, Test Accuracy: 30714.0000\n",
            "Epoch 5, Loss: 0.6969, Test Accuracy: 30675.0000\n",
            "Epoch 6, Loss: 0.6967, Test Accuracy: 30705.0000\n",
            "Epoch 7, Loss: 0.6961, Test Accuracy: 30708.0000\n",
            "Epoch 8, Loss: 0.6954, Test Accuracy: 30787.0000\n",
            "Epoch 9, Loss: 0.6947, Test Accuracy: 30792.0000\n",
            "Epoch 10, Loss: 0.6942, Test Accuracy: 30836.0000\n",
            "NDCG: 0.5905, Precision: 0.5021, Recall: 0.5020, F1-Score: 0.4962\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TGN-**houses**"
      ],
      "metadata": {
        "id": "2cD7fdFs_9L5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Mounting Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Import necessary libraries\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import networkx as nx\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch_geometric.data import Data, DataLoader\n",
        "import torch.optim as optim\n",
        "\n",
        "# Set environment variables\n",
        "os.environ['LC_ALL'] = 'en_US.UTF-8'\n",
        "\n",
        "# Load the dataset\n",
        "ratings = pd.read_csv('/content/drive/MyDrive/user_activity.csv')\n",
        "\n",
        "# Convert timestamp to datetime and sort by timestamp\n",
        "ratings['timestamp'] = pd.to_datetime(ratings['create_timestamp'])\n",
        "ratings = ratings.sort_values(by='timestamp')\n",
        "\n",
        "# Split the data into train and test sets\n",
        "train_data, test_data = train_test_split(ratings, test_size=0.2, shuffle=False)\n",
        "\n",
        "# Function to create graph from data\n",
        "def create_graph(data):\n",
        "    G = nx.DiGraph()\n",
        "    for _, row in data.iterrows():\n",
        "        # Use the timestamp() method to get a numeric timestamp\n",
        "        G.add_edge(row['user_id'], row['item_id'], timestamp=row['timestamp'].timestamp())\n",
        "    return G\n",
        "\n",
        "train_graph = create_graph(train_data)\n",
        "test_graph = create_graph(test_data)\n",
        "\n",
        "# Function to convert NetworkX graph to PyTorch Geometric Data object\n",
        "def convert_to_pyg_data(graph, num_features=8):\n",
        "    nodes = list(graph.nodes())\n",
        "    node_mapping = {node: i for i, node in enumerate(nodes)}\n",
        "    edge_index = torch.tensor([[node_mapping[u], node_mapping[v]] for u, v in graph.edges]).t().contiguous()\n",
        "    edge_time = torch.tensor([graph[u][v]['timestamp'] for u, v in graph.edges], dtype=torch.float)\n",
        "\n",
        "    # Create a feature matrix with fixed number of features per node\n",
        "    x = torch.randn(len(nodes), num_features)\n",
        "\n",
        "    # Random labels for the nodes (binary classification: 0 or 1)\n",
        "    y = torch.randint(0, 2, (len(nodes),))\n",
        "\n",
        "    data = Data(x=x, edge_index=edge_index, edge_time=edge_time, y=y)\n",
        "    return data\n",
        "\n",
        "train_data_pyg = convert_to_pyg_data(train_graph)\n",
        "test_data_pyg = convert_to_pyg_data(test_graph)\n",
        "\n",
        "train_loader = DataLoader([train_data_pyg], batch_size=1, shuffle=True)\n",
        "test_loader = DataLoader([test_data_pyg], batch_size=1, shuffle=False)\n",
        "\n",
        "# Custom TGN Model definition\n",
        "class TGNModel(torch.nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, memory_dim=8, time_dim=8):\n",
        "        super(TGNModel, self).__init__()\n",
        "        self.memory_dim = memory_dim\n",
        "        self.time_dim = time_dim\n",
        "\n",
        "        # Memory for each node\n",
        "        self.memory = torch.zeros(train_data_pyg.num_nodes, memory_dim)\n",
        "\n",
        "        # Embedding for time\n",
        "        self.time_embedding = torch.nn.Embedding(365, time_dim)\n",
        "\n",
        "        # Message and memory update functions\n",
        "        self.message_fn = torch.nn.Linear(in_channels + memory_dim + time_dim, memory_dim)\n",
        "        self.memory_update_fn = torch.nn.GRUCell(memory_dim, memory_dim)\n",
        "\n",
        "        # Final classification layer\n",
        "        self.fc = torch.nn.Linear(memory_dim, out_channels)\n",
        "\n",
        "    def forward(self, x, edge_index, edge_time):\n",
        "        # Get memory embeddings for source and target nodes\n",
        "        src, dst = edge_index\n",
        "        src_memory = self.memory[src]\n",
        "        dst_memory = self.memory[dst]\n",
        "\n",
        "        # Embed time\n",
        "        time_embeds = self.time_embedding((edge_time.long() % 365).view(-1, 1)).view(-1, self.time_dim)\n",
        "\n",
        "        # Create messages\n",
        "        messages = self.message_fn(torch.cat([x[src], src_memory, time_embeds], dim=1))\n",
        "\n",
        "        # Update memory for destination nodes (avoid in-place updates)\n",
        "        updated_memory = self.memory_update_fn(messages, dst_memory)\n",
        "        self.memory[dst] = updated_memory.detach()  # Detach to avoid breaking the computation graph\n",
        "\n",
        "        # Apply final classification layer\n",
        "        out = self.fc(updated_memory)  # Only output predictions for destination nodes\n",
        "        return out\n",
        "\n",
        "# Initialize the model, loss function, and optimizer\n",
        "model = TGNModel(in_channels=train_data_pyg.num_node_features, out_channels=2)\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
        "loss_fn = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "# Training function\n",
        "def train(model, loader, optimizer, loss_fn):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for data in loader:\n",
        "        optimizer.zero_grad()\n",
        "        out = model(data.x, data.edge_index, data.edge_time)  # TGN uses edge_time\n",
        "        loss = loss_fn(out, data.y[data.edge_index[1]])  # Only compute loss for destination nodes\n",
        "        loss.backward(retain_graph=True)  # Retain the computation graph\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "    return total_loss / len(loader)\n",
        "\n",
        "# Evaluation function\n",
        "def evaluate(model, loader):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for data in loader:\n",
        "        out = model(data.x, data.edge_index, data.edge_time)  # TGN uses edge_time\n",
        "        pred = out.argmax(dim=1)\n",
        "        correct += (pred == data.y[data.edge_index[1]]).sum().item()  # Only evaluate destination nodes\n",
        "        total += len(data.edge_index[1])\n",
        "    return correct / total\n",
        "\n",
        "# Training loop (now running for only 10 epochs)\n",
        "for epoch in range(10):  # Update: Loop only for 10 epochs\n",
        "    train_loss = train(model, train_loader, optimizer, loss_fn)\n",
        "    test_acc = evaluate(model, test_loader)\n",
        "    print(f'Epoch {epoch+1}, Loss: {train_loss:.4f}, Test Accuracy: {test_acc:.4f}')\n",
        "\n",
        "# Additional evaluation metrics\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
        "\n",
        "# Function to calculate MRR\n",
        "def mrr_score(y_true, y_pred):\n",
        "    order = np.argsort(y_pred)[::-1]\n",
        "    ranks = np.where(y_true[order] == 1)[0] + 1\n",
        "    return np.mean(1.0 / ranks)\n",
        "\n",
        "# Function to calculate NDCG\n",
        "def ndcg_score(y_true, y_pred, k=10):\n",
        "    order = np.argsort(y_pred)[::-1]\n",
        "    y_true = np.take(y_true, order[:k])\n",
        "\n",
        "    gains = 2 ** y_true - 1\n",
        "    discounts = np.log2(np.arange(2, k + 2))\n",
        "    dcg = np.sum(gains / discounts)\n",
        "\n",
        "    ideal_gains = 2 ** np.sort(y_true)[::-1] - 1\n",
        "    idcg = np.sum(ideal_gains / discounts)\n",
        "\n",
        "    return dcg / idcg if idcg > 0 else 0.0\n",
        "\n",
        "# Evaluation function with metrics\n",
        "def evaluate_with_metrics(model, loader):\n",
        "    model.eval()\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "    for data in loader:\n",
        "        out = model(data.x, data.edge_index, data.edge_time)  # TGN uses edge_time\n",
        "        pred = out.argmax(dim=1)\n",
        "        all_preds.append(pred.detach().cpu().numpy())\n",
        "        all_labels.append(data.y[data.edge_index[1]].detach().cpu().numpy())  # Only evaluate destination nodes\n",
        "\n",
        "    all_preds = np.concatenate(all_preds)\n",
        "    all_labels = np.concatenate(all_labels)\n",
        "\n",
        "    accuracy = accuracy_score(all_labels, all_preds)\n",
        "    precision = precision_score(all_labels, all_preds, average='macro')\n",
        "    recall = recall_score(all_labels, all_preds, average='macro')\n",
        "    f1 = f1_score(all_labels, all_preds, average='macro')\n",
        "    mrr = mrr_score(all_labels, all_preds)\n",
        "    ndcg = ndcg_score(all_labels, all_preds)\n",
        "\n",
        "    return accuracy, precision, recall, f1, mrr, ndcg\n",
        "\n",
        "# Final evaluation\n",
        "accuracy, precision, recall, f1, mrr, ndcg = evaluate_with_metrics(model, test_loader)\n",
        "print(f'NDCG: {ndcg:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}, F1-Score: {f1:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nfnaH8RpABgT",
        "outputId": "5f12ea26-f986-4de8-b4b0-1366c7f03823"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
            "  warnings.warn(out)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: 0.6937, Test Accuracy: 0.4981\n",
            "Epoch 2, Loss: 0.6930, Test Accuracy: 0.4966\n",
            "Epoch 3, Loss: 0.6933, Test Accuracy: 0.4968\n",
            "Epoch 4, Loss: 0.6934, Test Accuracy: 0.4967\n",
            "Epoch 5, Loss: 0.6930, Test Accuracy: 0.4962\n",
            "Epoch 6, Loss: 0.6925, Test Accuracy: 0.4946\n",
            "Epoch 7, Loss: 0.6923, Test Accuracy: 0.4930\n",
            "Epoch 8, Loss: 0.6922, Test Accuracy: 0.4931\n",
            "Epoch 9, Loss: 0.6919, Test Accuracy: 0.4952\n",
            "Epoch 10, Loss: 0.6917, Test Accuracy: 0.4959\n",
            "NDCG: 0.5261, Precision: 0.4909, Recall: 0.4990, F1-Score: 0.3537\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TGN-**movies**"
      ],
      "metadata": {
        "id": "6aIXZiNiTDsJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Mounting Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Import necessary libraries\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import networkx as nx\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch_geometric.data import Data, DataLoader\n",
        "import torch.optim as optim\n",
        "\n",
        "# Set environment variables\n",
        "os.environ['LC_ALL'] = 'en_US.UTF-8'\n",
        "\n",
        "# Load the dataset (using ratings_small.csv from MovieLens)\n",
        "ratings = pd.read_csv('/content/drive/MyDrive/movielens/ratings_small.csv')\n",
        "\n",
        "# Convert timestamp to datetime and sort by timestamp\n",
        "ratings['timestamp'] = pd.to_datetime(ratings['timestamp'])\n",
        "ratings = ratings.sort_values(by='timestamp')\n",
        "\n",
        "# Split the data into train and test sets\n",
        "train_data, test_data = train_test_split(ratings, test_size=0.2, shuffle=False)\n",
        "\n",
        "# Function to create graph from data\n",
        "def create_graph(data):\n",
        "    G = nx.DiGraph()\n",
        "    for _, row in data.iterrows():\n",
        "        # Use the timestamp() method to get a numeric timestamp\n",
        "        G.add_edge(row['userId'], row['movieId'], timestamp=row['timestamp'].timestamp())\n",
        "    return G\n",
        "\n",
        "train_graph = create_graph(train_data)\n",
        "test_graph = create_graph(test_data)\n",
        "\n",
        "# Function to convert NetworkX graph to PyTorch Geometric Data object\n",
        "def convert_to_pyg_data(graph, num_features=8):\n",
        "    nodes = list(graph.nodes())\n",
        "    node_mapping = {node: i for i, node in enumerate(nodes)}\n",
        "    edge_index = torch.tensor([[node_mapping[u], node_mapping[v]] for u, v in graph.edges]).t().contiguous()\n",
        "    edge_time = torch.tensor([graph[u][v]['timestamp'] for u, v in graph.edges], dtype=torch.float)\n",
        "\n",
        "    # Create a feature matrix with fixed number of features per node\n",
        "    x = torch.randn(len(nodes), num_features)\n",
        "\n",
        "    # Random labels for the nodes (binary classification: 0 or 1)\n",
        "    y = torch.randint(0, 2, (len(nodes),))\n",
        "\n",
        "    data = Data(x=x, edge_index=edge_index, edge_time=edge_time, y=y)\n",
        "    return data\n",
        "\n",
        "train_data_pyg = convert_to_pyg_data(train_graph)\n",
        "test_data_pyg = convert_to_pyg_data(test_graph)\n",
        "\n",
        "train_loader = DataLoader([train_data_pyg], batch_size=1, shuffle=True)\n",
        "test_loader = DataLoader([test_data_pyg], batch_size=1, shuffle=False)\n",
        "\n",
        "# Custom TGN Model definition\n",
        "class TGNModel(torch.nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, memory_dim=8, time_dim=8):\n",
        "        super(TGNModel, self).__init__()\n",
        "        self.memory_dim = memory_dim\n",
        "        self.time_dim = time_dim\n",
        "\n",
        "        # Memory for each node\n",
        "        self.memory = torch.zeros(train_data_pyg.num_nodes, memory_dim)\n",
        "\n",
        "        # Embedding for time\n",
        "        self.time_embedding = torch.nn.Embedding(365, time_dim)\n",
        "\n",
        "        # Message and memory update functions\n",
        "        self.message_fn = torch.nn.Linear(in_channels + memory_dim + time_dim, memory_dim)\n",
        "        self.memory_update_fn = torch.nn.GRUCell(memory_dim, memory_dim)\n",
        "\n",
        "        # Final classification layer\n",
        "        self.fc = torch.nn.Linear(memory_dim, out_channels)\n",
        "\n",
        "    def forward(self, x, edge_index, edge_time):\n",
        "        # Get memory embeddings for source and target nodes\n",
        "        src, dst = edge_index\n",
        "        src_memory = self.memory[src]\n",
        "        dst_memory = self.memory[dst]\n",
        "\n",
        "        # Embed time\n",
        "        time_embeds = self.time_embedding((edge_time.long() % 365).view(-1, 1)).view(-1, self.time_dim)\n",
        "\n",
        "        # Create messages\n",
        "        messages = self.message_fn(torch.cat([x[src], src_memory, time_embeds], dim=1))\n",
        "\n",
        "        # Update memory for destination nodes (avoid in-place updates)\n",
        "        updated_memory = self.memory_update_fn(messages, dst_memory)\n",
        "        self.memory[dst] = updated_memory.detach()  # Detach to avoid breaking the computation graph\n",
        "\n",
        "        # Apply final classification layer\n",
        "        out = self.fc(updated_memory)  # Only output predictions for destination nodes\n",
        "        return out\n",
        "\n",
        "# Initialize the model, loss function, and optimizer\n",
        "model = TGNModel(in_channels=train_data_pyg.num_node_features, out_channels=2)\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
        "loss_fn = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "# Training function\n",
        "def train(model, loader, optimizer, loss_fn):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for data in loader:\n",
        "        optimizer.zero_grad()\n",
        "        out = model(data.x, data.edge_index, data.edge_time)  # TGN uses edge_time\n",
        "        loss = loss_fn(out, data.y[data.edge_index[1]])  # Only compute loss for destination nodes\n",
        "        loss.backward(retain_graph=True)  # Retain the computation graph\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "    return total_loss / len(loader)\n",
        "\n",
        "# Evaluation function\n",
        "def evaluate(model, loader):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for data in loader:\n",
        "        out = model(data.x, data.edge_index, data.edge_time)  # TGN uses edge_time\n",
        "        pred = out.argmax(dim=1)\n",
        "        correct += (pred == data.y[data.edge_index[1]]).sum().item()  # Only evaluate destination nodes\n",
        "        total += len(data.edge_index[1])\n",
        "    return correct / total\n",
        "\n",
        "# Training loop (now running for only 10 epochs)\n",
        "for epoch in range(10):  # Update: Loop only for 10 epochs\n",
        "    train_loss = train(model, train_loader, optimizer, loss_fn)\n",
        "    test_acc = evaluate(model, test_loader)\n",
        "    print(f'Epoch {epoch+1}, Loss: {train_loss:.4f}, Test Accuracy: {test_acc:.4f}')\n",
        "\n",
        "# Additional evaluation metrics\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
        "\n",
        "# Function to calculate MRR\n",
        "def mrr_score(y_true, y_pred):\n",
        "    order = np.argsort(y_pred)[::-1]\n",
        "    ranks = np.where(y_true[order] == 1)[0] + 1\n",
        "    return np.mean(1.0 / ranks)\n",
        "\n",
        "# Function to calculate NDCG\n",
        "def ndcg_score(y_true, y_pred, k=10):\n",
        "    order = np.argsort(y_pred)[::-1]\n",
        "    y_true = np.take(y_true, order[:k])\n",
        "\n",
        "    gains = 2 ** y_true - 1\n",
        "    discounts = np.log2(np.arange(2, k + 2))\n",
        "    dcg = np.sum(gains / discounts)\n",
        "\n",
        "    ideal_gains = 2 ** np.sort(y_true)[::-1] - 1\n",
        "    idcg = np.sum(ideal_gains / discounts)\n",
        "\n",
        "    return dcg / idcg if idcg > 0 else 0.0\n",
        "\n",
        "# Evaluation function with metrics\n",
        "def evaluate_with_metrics(model, loader):\n",
        "    model.eval()\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "    for data in loader:\n",
        "        out = model(data.x, data.edge_index, data.edge_time)  # TGN uses edge_time\n",
        "        pred = out.argmax(dim=1)\n",
        "        all_preds.append(pred.detach().cpu().numpy())\n",
        "        all_labels.append(data.y[data.edge_index[1]].detach().cpu().numpy())  # Only evaluate destination nodes\n",
        "\n",
        "    all_preds = np.concatenate(all_preds)\n",
        "    all_labels = np.concatenate(all_labels)\n",
        "\n",
        "    accuracy = accuracy_score(all_labels, all_preds)\n",
        "    precision = precision_score(all_labels, all_preds, average='macro')\n",
        "    recall = recall_score(all_labels, all_preds, average='macro')\n",
        "    f1 = f1_score(all_labels, all_preds, average='macro')\n",
        "    mrr = mrr_score(all_labels, all_preds)\n",
        "    ndcg = ndcg_score(all_labels, all_preds)\n",
        "\n",
        "    return accuracy, precision, recall, f1, mrr, ndcg\n",
        "\n",
        "# Final evaluation\n",
        "accuracy, precision, recall, f1, mrr, ndcg = evaluate_with_metrics(model, test_loader)\n",
        "print(f'NDCG: {ndcg:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}, F1-Score: {f1:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xvAceLuUTHEC",
        "outputId": "816025b4-764e-4c63-b129-72614ca7aec2"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
            "  warnings.warn(out)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: 0.6965, Test Accuracy: 0.4896\n",
            "Epoch 2, Loss: 0.6950, Test Accuracy: 0.5115\n",
            "Epoch 3, Loss: 0.6943, Test Accuracy: 0.5161\n",
            "Epoch 4, Loss: 0.6957, Test Accuracy: 0.5163\n",
            "Epoch 5, Loss: 0.6953, Test Accuracy: 0.5163\n",
            "Epoch 6, Loss: 0.6938, Test Accuracy: 0.5161\n",
            "Epoch 7, Loss: 0.6929, Test Accuracy: 0.5087\n",
            "Epoch 8, Loss: 0.6930, Test Accuracy: 0.4982\n",
            "Epoch 9, Loss: 0.6936, Test Accuracy: 0.4957\n",
            "Epoch 10, Loss: 0.6938, Test Accuracy: 0.4982\n",
            "NDCG: 0.6031, Precision: 0.5020, Recall: 0.5020, F1-Score: 0.4952\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TGN-**amazon**"
      ],
      "metadata": {
        "id": "EQBVoA14yBRK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Import necessary libraries\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import networkx as nx\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch_geometric.data import Data, DataLoader\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Set environment variables\n",
        "os.environ['LC_ALL'] = 'en_US.UTF-8'\n",
        "\n",
        "# Load the Beauty dataset\n",
        "ratings = pd.read_csv('/content/drive/MyDrive/ratings_Beauty.csv')\n",
        "\n",
        "# Display the first few rows to check the structure of the dataset\n",
        "print(ratings.head())\n",
        "\n",
        "ratings['timestamp'] = pd.to_datetime(ratings['Timestamp'], unit='s')\n",
        "ratings = ratings.sort_values(by='timestamp')\n",
        "\n",
        "# Split the data into train and test sets\n",
        "train_data, test_data = train_test_split(ratings, test_size=0.2, shuffle=False)\n",
        "\n",
        "train_data = train_data.sample(frac=0.1, random_state=42)\n",
        "test_data = test_data.sample(frac=0.1, random_state=42)\n",
        "\n",
        "# Function to create graph from data\n",
        "def create_graph(data):\n",
        "    G = nx.DiGraph()\n",
        "    for _, row in data.iterrows():\n",
        "         G.add_edge(row['UserId'], row['ProductId'], timestamp=row['Timestamp'])\n",
        "    return G\n",
        "\n",
        "train_graph = create_graph(train_data)\n",
        "test_graph = create_graph(test_data)\n",
        "\n",
        "# Function to convert NetworkX graph to PyTorch Geometric Data object\n",
        "def convert_to_pyg_data(graph, num_features=8):\n",
        "    nodes = list(graph.nodes())\n",
        "    node_mapping = {node: i for i, node in enumerate(nodes)}\n",
        "    edge_index = torch.tensor([[node_mapping[u], node_mapping[v]] for u, v in graph.edges]).t().contiguous()\n",
        "    edge_time = torch.tensor([graph[u][v]['timestamp'] for u, v in graph.edges], dtype=torch.float)\n",
        "\n",
        "    # Create a feature matrix with fixed number of features per node\n",
        "    x = torch.randn(len(nodes), num_features)\n",
        "\n",
        "    # Random labels for the nodes (binary classification: 0 or 1)\n",
        "    y = torch.randint(0, 2, (len(nodes),))\n",
        "\n",
        "    data = Data(x=x, edge_index=edge_index, edge_time=edge_time, y=y)\n",
        "    return data\n",
        "\n",
        "train_data_pyg = convert_to_pyg_data(train_graph)\n",
        "test_data_pyg = convert_to_pyg_data(test_graph)\n",
        "\n",
        "train_loader = DataLoader([train_data_pyg], batch_size=1, shuffle=True)\n",
        "test_loader = DataLoader([test_data_pyg], batch_size=1, shuffle=False)\n",
        "\n",
        "# Custom TGN Model definition\n",
        "class TGNModel(torch.nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, memory_dim=8, time_dim=8):\n",
        "        super(TGNModel, self).__init__()\n",
        "        self.memory_dim = memory_dim\n",
        "        self.time_dim = time_dim\n",
        "\n",
        "        # Memory for each node\n",
        "        self.memory = torch.zeros(train_data_pyg.num_nodes, memory_dim)\n",
        "\n",
        "        # Embedding for time\n",
        "        self.time_embedding = torch.nn.Embedding(365, time_dim)\n",
        "\n",
        "        # Message and memory update functions\n",
        "        self.message_fn = torch.nn.Linear(in_channels + memory_dim + time_dim, memory_dim)\n",
        "        self.memory_update_fn = torch.nn.GRUCell(memory_dim, memory_dim)\n",
        "\n",
        "        # Final classification layer\n",
        "        self.fc = torch.nn.Linear(memory_dim, out_channels)\n",
        "\n",
        "    def forward(self, x, edge_index, edge_time):\n",
        "        # Get memory embeddings for source and target nodes\n",
        "        src, dst = edge_index\n",
        "        src_memory = self.memory[src]\n",
        "        dst_memory = self.memory[dst]\n",
        "\n",
        "        # Embed time\n",
        "        time_embeds = self.time_embedding((edge_time.long() % 365).view(-1, 1)).view(-1, self.time_dim)\n",
        "\n",
        "        # Create messages\n",
        "        messages = self.message_fn(torch.cat([x[src], src_memory, time_embeds], dim=1))\n",
        "\n",
        "        # Update memory for destination nodes (avoid in-place updates)\n",
        "        updated_memory = self.memory_update_fn(messages, dst_memory)\n",
        "        self.memory[dst] = updated_memory.detach()  # Detach to avoid breaking the computation graph\n",
        "\n",
        "        # Apply final classification layer\n",
        "        out = self.fc(updated_memory)  # Only output predictions for destination nodes\n",
        "        return out\n",
        "\n",
        "# Initialize the model, loss function, and optimizer\n",
        "model = TGNModel(in_channels=train_data_pyg.num_node_features, out_channels=2)\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
        "loss_fn = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "# Training function\n",
        "def train(model, loader, optimizer, loss_fn):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for data in loader:\n",
        "        optimizer.zero_grad()\n",
        "        out = model(data.x, data.edge_index, data.edge_time)  # TGN uses edge_time\n",
        "        loss = loss_fn(out, data.y[data.edge_index[1]])  # Only compute loss for destination nodes\n",
        "        loss.backward(retain_graph=True)  # Retain the computation graph\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "    return total_loss / len(loader)\n",
        "\n",
        "# Evaluation function\n",
        "def evaluate(model, loader):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for data in loader:\n",
        "        out = model(data.x, data.edge_index, data.edge_time)  # TGN uses edge_time\n",
        "        pred = out.argmax(dim=1)\n",
        "        correct += (pred == data.y[data.edge_index[1]]).sum().item()  # Only evaluate destination nodes\n",
        "        total += len(data.edge_index[1])\n",
        "    return correct / total\n",
        "\n",
        "# Training loop (now running for only 10 epochs)\n",
        "for epoch in range(10):  # Update: Loop only for 10 epochs\n",
        "    train_loss = train(model, train_loader, optimizer, loss_fn)\n",
        "    test_acc = evaluate(model, test_loader)\n",
        "    print(f'Epoch {epoch+1}, Loss: {train_loss:.4f}, Test Accuracy: {test_acc:.4f}')\n",
        "\n",
        "# Additional evaluation metrics\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
        "\n",
        "# Function to calculate MRR\n",
        "def mrr_score(y_true, y_pred):\n",
        "    order = np.argsort(y_pred)[::-1]\n",
        "    ranks = np.where(y_true[order] == 1)[0] + 1\n",
        "    return np.mean(1.0 / ranks)\n",
        "\n",
        "# Function to calculate NDCG\n",
        "def ndcg_score(y_true, y_pred, k=10):\n",
        "    order = np.argsort(y_pred)[::-1]\n",
        "    y_true = np.take(y_true, order[:k])\n",
        "\n",
        "    gains = 2 ** y_true - 1\n",
        "    discounts = np.log2(np.arange(2, k + 2))\n",
        "    dcg = np.sum(gains / discounts)\n",
        "\n",
        "    ideal_gains = 2 ** np.sort(y_true)[::-1] - 1\n",
        "    idcg = np.sum(ideal_gains / discounts)\n",
        "\n",
        "    return dcg / idcg if idcg > 0 else 0.0\n",
        "\n",
        "# Evaluation function with metrics\n",
        "def evaluate_with_metrics(model, loader):\n",
        "    model.eval()\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "    for data in loader:\n",
        "        out = model(data.x, data.edge_index, data.edge_time)  # TGN uses edge_time\n",
        "        pred = out.argmax(dim=1)\n",
        "        all_preds.append(pred.detach().cpu().numpy())\n",
        "        all_labels.append(data.y[data.edge_index[1]].detach().cpu().numpy())  # Only evaluate destination nodes\n",
        "\n",
        "    all_preds = np.concatenate(all_preds)\n",
        "    all_labels = np.concatenate(all_labels)\n",
        "\n",
        "    accuracy = accuracy_score(all_labels, all_preds)\n",
        "    precision = precision_score(all_labels, all_preds, average='macro')\n",
        "    recall = recall_score(all_labels, all_preds, average='macro')\n",
        "    f1 = f1_score(all_labels, all_preds, average='macro')\n",
        "    mrr = mrr_score(all_labels, all_preds)\n",
        "    ndcg = ndcg_score(all_labels, all_preds)\n",
        "\n",
        "    return accuracy, precision, recall, f1, mrr, ndcg\n",
        "\n",
        "# Final evaluation\n",
        "accuracy, precision, recall, f1, mrr, ndcg = evaluate_with_metrics(model, test_loader)\n",
        "print(f'NDCG: {ndcg:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}, F1-Score: {f1:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2O1U1JYjyD3g",
        "outputId": "e567abd1-a78a-49fd-90d8-f762e2fb7d60"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "           UserId   ProductId  Rating   Timestamp\n",
            "0  A39HTATAQ9V7YF  0205616461     5.0  1369699200\n",
            "1  A3JM6GV9MNOF9X  0558925278     3.0  1355443200\n",
            "2  A1Z513UWSAAO0F  0558925278     5.0  1404691200\n",
            "3  A1WMRR494NWEWV  0733001998     4.0  1382572800\n",
            "4  A3IAAVS479H7M7  0737104473     1.0  1274227200\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
            "  warnings.warn(out)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: 0.7183, Test Accuracy: 0.4933\n",
            "Epoch 2, Loss: 0.7154, Test Accuracy: 0.4938\n",
            "Epoch 3, Loss: 0.7077, Test Accuracy: 0.4946\n",
            "Epoch 4, Loss: 0.7012, Test Accuracy: 0.4989\n",
            "Epoch 5, Loss: 0.6970, Test Accuracy: 0.5008\n",
            "Epoch 6, Loss: 0.6950, Test Accuracy: 0.5029\n",
            "Epoch 7, Loss: 0.6945, Test Accuracy: 0.5024\n",
            "Epoch 8, Loss: 0.6947, Test Accuracy: 0.5056\n",
            "Epoch 9, Loss: 0.6952, Test Accuracy: 0.5064\n",
            "Epoch 10, Loss: 0.6955, Test Accuracy: 0.5074\n",
            "NDCG: 0.4401, Precision: 0.5036, Recall: 0.5017, F1-Score: 0.4298\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# RNN-**houses**"
      ],
      "metadata": {
        "id": "EWFRVEhVNNtQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Mounting Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Import necessary libraries\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import networkx as nx\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch_geometric.data import Data, DataLoader\n",
        "import torch.optim as optim\n",
        "from torch.nn import RNN, Linear\n",
        "\n",
        "# Set environment variables\n",
        "os.environ['LC_ALL'] = 'en_US.UTF-8'\n",
        "\n",
        "# Load the dataset\n",
        "ratings = pd.read_csv('/content/drive/MyDrive/user_activity.csv')\n",
        "\n",
        "# Convert timestamp to datetime and sort by timestamp\n",
        "ratings['timestamp'] = pd.to_datetime(ratings['create_timestamp'])\n",
        "ratings = ratings.sort_values(by='timestamp')\n",
        "\n",
        "# Split the data into train and test sets\n",
        "train_data, test_data = train_test_split(ratings, test_size=0.2, shuffle=False)\n",
        "\n",
        "# Function to create graph from data\n",
        "def create_graph(data):\n",
        "    G = nx.DiGraph()\n",
        "    for _, row in data.iterrows():\n",
        "        # Use the timestamp() method to get a numeric timestamp\n",
        "        G.add_edge(row['user_id'], row['item_id'], timestamp=row['timestamp'].timestamp())\n",
        "    return G\n",
        "\n",
        "train_graph = create_graph(train_data)\n",
        "test_graph = create_graph(test_data)\n",
        "\n",
        "# Function to convert NetworkX graph to PyTorch Geometric Data object\n",
        "def convert_to_pyg_data(graph, num_features=8):\n",
        "    nodes = list(graph.nodes())\n",
        "    node_mapping = {node: i for i, node in enumerate(nodes)}\n",
        "    edge_index = torch.tensor([[node_mapping[u], node_mapping[v]] for u, v in graph.edges]).t().contiguous()\n",
        "    edge_time = torch.tensor([graph[u][v]['timestamp'] for u, v in graph.edges], dtype=torch.float)\n",
        "\n",
        "    # Create a feature matrix with fixed number of features per node\n",
        "    x = torch.randn(len(nodes), num_features)\n",
        "\n",
        "    # Random labels for the nodes (binary classification: 0 or 1)\n",
        "    y = torch.randint(0, 2, (len(nodes),))\n",
        "\n",
        "    data = Data(x=x, edge_index=edge_index, edge_time=edge_time, y=y)\n",
        "    return data\n",
        "\n",
        "train_data_pyg = convert_to_pyg_data(train_graph)\n",
        "test_data_pyg = convert_to_pyg_data(test_graph)\n",
        "\n",
        "train_loader = DataLoader([train_data_pyg], batch_size=1, shuffle=True)\n",
        "test_loader = DataLoader([test_data_pyg], batch_size=1, shuffle=False)\n",
        "\n",
        "# RNN Model definition\n",
        "class RNNModel(torch.nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        super(RNNModel, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.rnn = RNN(input_size, hidden_size, batch_first=True)\n",
        "        self.fc = Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, x, edge_index, edge_time):\n",
        "        # Prepare input for RNN\n",
        "        # x: Node features (num_nodes, num_features)\n",
        "        # edge_index: Edge connections (2, num_edges)\n",
        "        # edge_time: Timestamps for edges (num_edges,)\n",
        "\n",
        "        # Sort edges by timestamp\n",
        "        sorted_indices = torch.argsort(edge_time)\n",
        "        sorted_edge_index = edge_index[:, sorted_indices]\n",
        "        sorted_edge_time = edge_time[sorted_indices]\n",
        "\n",
        "        # Prepare sequences for RNN\n",
        "        src, dst = sorted_edge_index\n",
        "        sequences = x[src]  # Use source node features as input sequences\n",
        "\n",
        "        # Initialize hidden state with correct batch size\n",
        "        batch_size = sequences.size(0)  # Number of edges\n",
        "        h0 = torch.zeros(1, batch_size, self.hidden_size)  # (num_layers, batch_size, hidden_size)\n",
        "\n",
        "        # Pass sequences through RNN\n",
        "        out, _ = self.rnn(sequences.unsqueeze(1), h0)  # Add sequence length dimension\n",
        "        out = out.squeeze(1)  # Remove sequence length dimension\n",
        "\n",
        "        # Apply final classification layer\n",
        "        out = self.fc(out)\n",
        "        return out  # Output predictions for all edges\n",
        "\n",
        "# Initialize the model, loss function, and optimizer\n",
        "input_size = train_data_pyg.num_node_features\n",
        "hidden_size = 16\n",
        "output_size = 2\n",
        "model = RNNModel(input_size, hidden_size, output_size)\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
        "loss_fn = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "# Training function\n",
        "def train(model, loader, optimizer, loss_fn):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for data in loader:\n",
        "        optimizer.zero_grad()\n",
        "        out = model(data.x, data.edge_index, data.edge_time)  # RNN uses edge_time\n",
        "        loss = loss_fn(out, data.y[data.edge_index[1]])  # Only compute loss for destination nodes\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "    return total_loss / len(loader)\n",
        "\n",
        "# Evaluation function\n",
        "def evaluate(model, loader):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for data in loader:\n",
        "        out = model(data.x, data.edge_index, data.edge_time)  # RNN uses edge_time\n",
        "        pred = out.argmax(dim=1)\n",
        "        correct += (pred == data.y[data.edge_index[1]]).sum().item()  # Only evaluate destination nodes\n",
        "        total += len(data.edge_index[1])\n",
        "    return correct / total\n",
        "\n",
        "# Training loop (now running for only 10 epochs)\n",
        "for epoch in range(10):  # Update: Loop only for 10 epochs\n",
        "    train_loss = train(model, train_loader, optimizer, loss_fn)\n",
        "    test_acc = evaluate(model, test_loader)\n",
        "    print(f'Epoch {epoch+1}, Loss: {train_loss:.4f}, Test Accuracy: {test_acc:.4f}')\n",
        "\n",
        "# Additional evaluation metrics\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
        "\n",
        "# Function to calculate MRR\n",
        "def mrr_score(y_true, y_pred):\n",
        "    order = np.argsort(y_pred)[::-1]\n",
        "    ranks = np.where(y_true[order] == 1)[0] + 1\n",
        "    return np.mean(1.0 / ranks)\n",
        "\n",
        "# Function to calculate NDCG\n",
        "def ndcg_score(y_true, y_pred, k=10):\n",
        "    order = np.argsort(y_pred)[::-1]\n",
        "    y_true = np.take(y_true, order[:k])\n",
        "\n",
        "    gains = 2 ** y_true - 1\n",
        "    discounts = np.log2(np.arange(2, k + 2))\n",
        "    dcg = np.sum(gains / discounts)\n",
        "\n",
        "    ideal_gains = 2 ** np.sort(y_true)[::-1] - 1\n",
        "    idcg = np.sum(ideal_gains / discounts)\n",
        "\n",
        "    return dcg / idcg if idcg > 0 else 0.0\n",
        "\n",
        "# Evaluation function with metrics\n",
        "def evaluate_with_metrics(model, loader):\n",
        "    model.eval()\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "    for data in loader:\n",
        "        out = model(data.x, data.edge_index, data.edge_time)  # RNN uses edge_time\n",
        "        pred = out.argmax(dim=1)\n",
        "        all_preds.append(pred.detach().cpu().numpy())\n",
        "        all_labels.append(data.y[data.edge_index[1]].detach().cpu().numpy())  # Only evaluate destination nodes\n",
        "\n",
        "    all_preds = np.concatenate(all_preds)\n",
        "    all_labels = np.concatenate(all_labels)\n",
        "\n",
        "    accuracy = accuracy_score(all_labels, all_preds)\n",
        "    precision = precision_score(all_labels, all_preds, average='macro')\n",
        "    recall = recall_score(all_labels, all_preds, average='macro')\n",
        "    f1 = f1_score(all_labels, all_preds, average='macro')\n",
        "    mrr = mrr_score(all_labels, all_preds)\n",
        "    ndcg = ndcg_score(all_labels, all_preds)\n",
        "\n",
        "    return accuracy, precision, recall, f1, mrr, ndcg\n",
        "\n",
        "# Final evaluation\n",
        "accuracy, precision, recall, f1, mrr, ndcg = evaluate_with_metrics(model, test_loader)\n",
        "print(f'NDCG: {ndcg:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}, F1-Score: {f1:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T-tEUl-PNQjo",
        "outputId": "bbe20b5b-11ae-4dc5-9723-f9ebfe629179"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
            "  warnings.warn(out)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: 0.7266, Test Accuracy: 0.4842\n",
            "Epoch 2, Loss: 0.7121, Test Accuracy: 0.4865\n",
            "Epoch 3, Loss: 0.7025, Test Accuracy: 0.4959\n",
            "Epoch 4, Loss: 0.6971, Test Accuracy: 0.5074\n",
            "Epoch 5, Loss: 0.6949, Test Accuracy: 0.5232\n",
            "Epoch 6, Loss: 0.6947, Test Accuracy: 0.5255\n",
            "Epoch 7, Loss: 0.6952, Test Accuracy: 0.5254\n",
            "Epoch 8, Loss: 0.6955, Test Accuracy: 0.5240\n",
            "Epoch 9, Loss: 0.6954, Test Accuracy: 0.5214\n",
            "Epoch 10, Loss: 0.6952, Test Accuracy: 0.5181\n",
            "NDCG: 0.4790, Precision: 0.5031, Recall: 0.5019, F1-Score: 0.4582\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# RNN-**movies**"
      ],
      "metadata": {
        "id": "PkzNa14-Tlix"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Import necessary libraries\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import networkx as nx\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch_geometric.data import Data, DataLoader\n",
        "import torch.optim as optim\n",
        "from torch.nn import RNN, Linear\n",
        "\n",
        "# Set environment variables\n",
        "os.environ['LC_ALL'] = 'en_US.UTF-8'\n",
        "\n",
        "# Load the dataset\n",
        "ratings = pd.read_csv('/content/drive/MyDrive/movielens/ratings_small.csv')\n",
        "\n",
        "# Convert timestamp to datetime and sort by timestamp\n",
        "ratings['timestamp'] = pd.to_datetime(ratings['timestamp'])\n",
        "ratings = ratings.sort_values(by='timestamp')\n",
        "\n",
        "# Split the data into train and test sets\n",
        "train_data, test_data = train_test_split(ratings, test_size=0.2, shuffle=False)\n",
        "\n",
        "# Function to create graph from data\n",
        "def create_graph(data):\n",
        "    G = nx.DiGraph()\n",
        "    for _, row in data.iterrows():\n",
        "        G.add_edge(row['userId'], row['movieId'], timestamp=row['timestamp'].timestamp())\n",
        "    return G\n",
        "\n",
        "train_graph = create_graph(train_data)\n",
        "test_graph = create_graph(test_data)\n",
        "\n",
        "# Function to convert NetworkX graph to PyTorch Geometric Data object\n",
        "def convert_to_pyg_data(graph, num_features=8):\n",
        "    nodes = list(graph.nodes())\n",
        "    node_mapping = {node: i for i, node in enumerate(nodes)}\n",
        "    edge_index = torch.tensor([[node_mapping[u], node_mapping[v]] for u, v in graph.edges]).t().contiguous()\n",
        "    edge_time = torch.tensor([graph[u][v]['timestamp'] for u, v in graph.edges], dtype=torch.float)\n",
        "\n",
        "    # Create a feature matrix with fixed number of features per node\n",
        "    x = torch.randn(len(nodes), num_features)\n",
        "\n",
        "    # Random labels for the nodes (binary classification: 0 or 1)\n",
        "    y = torch.randint(0, 2, (len(nodes),))\n",
        "\n",
        "    data = Data(x=x, edge_index=edge_index, edge_time=edge_time, y=y)\n",
        "    return data\n",
        "\n",
        "train_data_pyg = convert_to_pyg_data(train_graph)\n",
        "test_data_pyg = convert_to_pyg_data(test_graph)\n",
        "\n",
        "train_loader = DataLoader([train_data_pyg], batch_size=1, shuffle=True)\n",
        "test_loader = DataLoader([test_data_pyg], batch_size=1, shuffle=False)\n",
        "\n",
        "# RNN Model definition\n",
        "class RNNModel(torch.nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        super(RNNModel, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.rnn = RNN(input_size, hidden_size, batch_first=True)\n",
        "        self.fc = Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, x, edge_index, edge_time):\n",
        "        # Sort edges by timestamp\n",
        "        sorted_indices = torch.argsort(edge_time)\n",
        "        sorted_edge_index = edge_index[:, sorted_indices]\n",
        "        sorted_edge_time = edge_time[sorted_indices]\n",
        "\n",
        "        # Prepare sequences for RNN\n",
        "        src, dst = sorted_edge_index\n",
        "        sequences = x[src]  # Use source node features as input sequences\n",
        "\n",
        "        # Initialize hidden state with correct batch size\n",
        "        batch_size = sequences.size(0)  # Number of edges\n",
        "        h0 = torch.zeros(1, batch_size, self.hidden_size)  # (num_layers, batch_size, hidden_size)\n",
        "\n",
        "        # Pass sequences through RNN\n",
        "        out, _ = self.rnn(sequences.unsqueeze(1), h0)  # Add sequence length dimension\n",
        "        out = out.squeeze(1)  # Remove sequence length dimension\n",
        "\n",
        "        # Apply final classification layer\n",
        "        out = self.fc(out)\n",
        "        return out  # Output predictions for all edges\n",
        "\n",
        "# Initialize the model, loss function, and optimizer\n",
        "input_size = train_data_pyg.num_node_features\n",
        "hidden_size = 16\n",
        "output_size = 2\n",
        "model = RNNModel(input_size, hidden_size, output_size)\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
        "loss_fn = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "# Training function\n",
        "def train(model, loader, optimizer, loss_fn):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for data in loader:\n",
        "        optimizer.zero_grad()\n",
        "        out = model(data.x, data.edge_index, data.edge_time)  # RNN uses edge_time\n",
        "        loss = loss_fn(out, data.y[data.edge_index[1]])  # Only compute loss for destination nodes\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "    return total_loss / len(loader)\n",
        "\n",
        "# Evaluation function\n",
        "def evaluate(model, loader):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for data in loader:\n",
        "        out = model(data.x, data.edge_index, data.edge_time)  # RNN uses edge_time\n",
        "        pred = out.argmax(dim=1)\n",
        "        correct += (pred == data.y[data.edge_index[1]]).sum().item()  # Only evaluate destination nodes\n",
        "        total += len(data.edge_index[1])\n",
        "    return correct / total\n",
        "\n",
        "# Training loop (now running for only 10 epochs)\n",
        "for epoch in range(10):  # Update: Loop only for 10 epochs\n",
        "    train_loss = train(model, train_loader, optimizer, loss_fn)\n",
        "    test_acc = evaluate(model, test_loader)\n",
        "    print(f'Epoch {epoch+1}, Loss: {train_loss:.4f}, Test Accuracy: {test_acc:.4f}')\n",
        "\n",
        "# Additional evaluation metrics\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
        "\n",
        "# Function to calculate MRR\n",
        "def mrr_score(y_true, y_pred):\n",
        "    order = np.argsort(y_pred)[::-1]\n",
        "    ranks = np.where(y_true[order] == 1)[0] + 1\n",
        "    return np.mean(1.0 / ranks)\n",
        "\n",
        "# Function to calculate NDCG\n",
        "def ndcg_score(y_true, y_pred, k=10):\n",
        "    order = np.argsort(y_pred)[::-1]\n",
        "    y_true = np.take(y_true, order[:k])\n",
        "\n",
        "    gains = 2 ** y_true - 1\n",
        "    discounts = np.log2(np.arange(2, k + 2))\n",
        "    dcg = np.sum(gains / discounts)\n",
        "\n",
        "    ideal_gains = 2 ** np.sort(y_true)[::-1] - 1\n",
        "    idcg = np.sum(ideal_gains / discounts)\n",
        "\n",
        "    return dcg / idcg if idcg > 0 else 0.0\n",
        "\n",
        "# Evaluation function with metrics\n",
        "def evaluate_with_metrics(model, loader):\n",
        "    model.eval()\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "    for data in loader:\n",
        "        out = model(data.x, data.edge_index, data.edge_time)  # RNN uses edge_time\n",
        "        pred = out.argmax(dim=1)\n",
        "        all_preds.append(pred.detach().cpu().numpy())\n",
        "        all_labels.append(data.y[data.edge_index[1]].detach().cpu().numpy())  # Only evaluate destination nodes\n",
        "\n",
        "    all_preds = np.concatenate(all_preds)\n",
        "    all_labels = np.concatenate(all_labels)\n",
        "\n",
        "    accuracy = accuracy_score(all_labels, all_preds)\n",
        "    precision = precision_score(all_labels, all_preds, average='macro')\n",
        "    recall = recall_score(all_labels, all_preds, average='macro')\n",
        "    f1 = f1_score(all_labels, all_preds, average='macro')\n",
        "    mrr = mrr_score(all_labels, all_preds)\n",
        "    ndcg = ndcg_score(all_labels, all_preds)\n",
        "\n",
        "    return accuracy, precision, recall, f1, mrr, ndcg\n",
        "\n",
        "# Final evaluation\n",
        "accuracy, precision, recall, f1, mrr, ndcg = evaluate_with_metrics(model, test_loader)\n",
        "print(f'NDCG: {ndcg:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}, F1-Score: {f1:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KVCWyfbCTorO",
        "outputId": "ac938522-545e-4666-a328-5ac1f052318c"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
            "  warnings.warn(out)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: 0.7251, Test Accuracy: 0.5158\n",
            "Epoch 2, Loss: 0.7096, Test Accuracy: 0.5149\n",
            "Epoch 3, Loss: 0.6995, Test Accuracy: 0.5021\n",
            "Epoch 4, Loss: 0.6947, Test Accuracy: 0.4916\n",
            "Epoch 5, Loss: 0.6940, Test Accuracy: 0.4900\n",
            "Epoch 6, Loss: 0.6957, Test Accuracy: 0.4861\n",
            "Epoch 7, Loss: 0.6976, Test Accuracy: 0.4854\n",
            "Epoch 8, Loss: 0.6986, Test Accuracy: 0.4845\n",
            "Epoch 9, Loss: 0.6983, Test Accuracy: 0.4845\n",
            "Epoch 10, Loss: 0.6971, Test Accuracy: 0.4842\n",
            "NDCG: 0.5668, Precision: 0.4900, Recall: 0.4992, F1-Score: 0.3418\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# RNN-**amazon**"
      ],
      "metadata": {
        "id": "RjBSn9TWy5G8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Mounting Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Import necessary libraries\n",
        "import os\n",
        "import pandas as pd\n",
        "import networkx as nx\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch_geometric.data import Data, DataLoader\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
        "\n",
        "# Set environment variables\n",
        "os.environ['LC_ALL'] = 'en_US.UTF-8'\n",
        "\n",
        "# Load the Beauty dataset\n",
        "ratings = pd.read_csv('/content/drive/MyDrive/ratings_Beauty.csv')\n",
        "\n",
        "# Display the first few rows to check the structure of the dataset\n",
        "print(ratings.head())\n",
        "\n",
        "ratings['timestamp'] = pd.to_datetime(ratings['Timestamp'], unit='s')\n",
        "ratings = ratings.sort_values(by='timestamp')\n",
        "\n",
        "# Split the data into train and test sets\n",
        "train_data, test_data = train_test_split(ratings, test_size=0.2, shuffle=False)\n",
        "\n",
        "train_data = train_data.sample(frac=0.1, random_state=42)\n",
        "test_data = test_data.sample(frac=0.1, random_state=42)\n",
        "\n",
        "# Function to create graph from data\n",
        "def create_graph(data):\n",
        "    G = nx.DiGraph()\n",
        "    for _, row in data.iterrows():\n",
        "        # Use the timestamp() method to get a numeric timestamp\n",
        "        G.add_edge(row['UserId'], row['ProductId'], timestamp=row['Timestamp'])\n",
        "    return G\n",
        "\n",
        "# Create graphs for train and test data\n",
        "train_graph = create_graph(train_data)\n",
        "test_graph = create_graph(test_data)\n",
        "\n",
        "# Function to convert NetworkX graph to PyTorch Geometric Data object\n",
        "def convert_to_pyg_data(graph, num_features=8):\n",
        "    nodes = list(graph.nodes())\n",
        "    node_mapping = {node: i for i, node in enumerate(nodes)}\n",
        "    edge_index = torch.tensor([[node_mapping[u], node_mapping[v]] for u, v in graph.edges]).t().contiguous()\n",
        "    edge_time = torch.tensor([graph[u][v]['timestamp'] for u, v in graph.edges], dtype=torch.float)\n",
        "\n",
        "    # Create a feature matrix with fixed number of features per node\n",
        "    x = torch.randn(len(nodes), num_features)\n",
        "\n",
        "    # Random labels for the nodes (binary classification: 0 or 1)\n",
        "    y = torch.randint(0, 2, (len(nodes),))\n",
        "\n",
        "    data = Data(x=x, edge_index=edge_index, edge_time=edge_time, y=y)\n",
        "    return data\n",
        "\n",
        "# Convert graphs to PyTorch Geometric Data objects\n",
        "train_data_pyg = convert_to_pyg_data(train_graph)\n",
        "test_data_pyg = convert_to_pyg_data(test_graph)\n",
        "\n",
        "# Create DataLoader instances for batch processing\n",
        "train_loader = DataLoader([train_data_pyg], batch_size=1, shuffle=True)\n",
        "test_loader = DataLoader([test_data_pyg], batch_size=1, shuffle=False)\n",
        "\n",
        "# RNN Model definition\n",
        "class RNNModel(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        super(RNNModel, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.rnn = nn.RNN(input_size, hidden_size, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, x, edge_index, edge_time):\n",
        "        # Sort edges by timestamp\n",
        "        sorted_indices = torch.argsort(edge_time)\n",
        "        sorted_edge_index = edge_index[:, sorted_indices]\n",
        "        sorted_edge_time = edge_time[sorted_indices]\n",
        "\n",
        "        # Prepare sequences for RNN\n",
        "        src, dst = sorted_edge_index\n",
        "        sequences = x[src]  # Use source node features as input sequences\n",
        "\n",
        "        # Initialize hidden state with correct batch size\n",
        "        batch_size = sequences.size(0)  # Number of edges\n",
        "        h0 = torch.zeros(1, batch_size, self.hidden_size)  # (num_layers, batch_size, hidden_size)\n",
        "\n",
        "        # Pass sequences through RNN\n",
        "        out, _ = self.rnn(sequences.unsqueeze(1), h0)  # Add sequence length dimension\n",
        "        out = out.squeeze(1)  # Remove sequence length dimension\n",
        "\n",
        "        # Apply final classification layer\n",
        "        out = self.fc(out)\n",
        "        return out  # Output predictions for all edges\n",
        "\n",
        "# Initialize the model, loss function, and optimizer\n",
        "input_size = train_data_pyg.num_node_features\n",
        "hidden_size = 16\n",
        "output_size = 2\n",
        "model = RNNModel(input_size, hidden_size, output_size)\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "# Training function\n",
        "def train(model, loader, optimizer, loss_fn):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for data in loader:\n",
        "        optimizer.zero_grad()\n",
        "        out = model(data.x, data.edge_index, data.edge_time)  # RNN uses edge_time\n",
        "        loss = loss_fn(out, data.y[data.edge_index[1]])  # Only compute loss for destination nodes\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "    return total_loss / len(loader)\n",
        "\n",
        "# Evaluation function\n",
        "def evaluate(model, loader):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for data in loader:\n",
        "        out = model(data.x, data.edge_index, data.edge_time)  # RNN uses edge_time\n",
        "        pred = out.argmax(dim=1)\n",
        "        correct += (pred == data.y[data.edge_index[1]]).sum().item()  # Only evaluate destination nodes\n",
        "        total += len(data.edge_index[1])\n",
        "    return correct / total\n",
        "\n",
        "# Training loop (now running for only 10 epochs)\n",
        "for epoch in range(10):  # Update: Loop only for 10 epochs\n",
        "    train_loss = train(model, train_loader, optimizer, loss_fn)\n",
        "    test_acc = evaluate(model, test_loader)\n",
        "    print(f'Epoch {epoch+1}, Loss: {train_loss:.4f}, Test Accuracy: {test_acc:.4f}')\n",
        "\n",
        "# Additional evaluation metrics\n",
        "# Function to calculate MRR\n",
        "def mrr_score(y_true, y_pred):\n",
        "    order = np.argsort(y_pred)[::-1]\n",
        "    ranks = np.where(y_true[order] == 1)[0] + 1\n",
        "    return np.mean(1.0 / ranks)\n",
        "\n",
        "# Function to calculate NDCG\n",
        "def ndcg_score(y_true, y_pred, k=10):\n",
        "    order = np.argsort(y_pred)[::-1]\n",
        "    y_true = np.take(y_true, order[:k])\n",
        "\n",
        "    gains = 2 ** y_true - 1\n",
        "    discounts = np.log2(np.arange(2, k + 2))\n",
        "    dcg = np.sum(gains / discounts)\n",
        "\n",
        "    ideal_gains = 2 ** np.sort(y_true)[::-1] - 1\n",
        "    idcg = np.sum(ideal_gains / discounts)\n",
        "\n",
        "    return dcg / idcg if idcg > 0 else 0.0\n",
        "\n",
        "# Evaluation function with metrics\n",
        "def evaluate_with_metrics(model, loader):\n",
        "    model.eval()\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "    for data in loader:\n",
        "        out = model(data.x, data.edge_index, data.edge_time)  # RNN uses edge_time\n",
        "        pred = out.argmax(dim=1)\n",
        "        all_preds.append(pred.detach().cpu().numpy())\n",
        "        all_labels.append(data.y[data.edge_index[1]].detach().cpu().numpy())  # Only evaluate destination nodes\n",
        "\n",
        "    all_preds = np.concatenate(all_preds)\n",
        "    all_labels = np.concatenate(all_labels)\n",
        "\n",
        "    accuracy = accuracy_score(all_labels, all_preds)\n",
        "    precision = precision_score(all_labels, all_preds, average='macro')\n",
        "    recall = recall_score(all_labels, all_preds, average='macro')\n",
        "    f1 = f1_score(all_labels, all_preds, average='macro')\n",
        "    mrr = mrr_score(all_labels, all_preds)\n",
        "    ndcg = ndcg_score(all_labels, all_preds)\n",
        "\n",
        "    return accuracy, precision, recall, f1, mrr, ndcg\n",
        "\n",
        "# Final evaluation\n",
        "accuracy, precision, recall, f1, mrr, ndcg = evaluate_with_metrics(model, test_loader)\n",
        "print(f'NDCG: {ndcg:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}, F1-Score: {f1:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "naEeEq3gy9cV",
        "outputId": "bbe9d9f9-fd37-426e-9ae7-5820db089f94"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "           UserId   ProductId  Rating   Timestamp\n",
            "0  A39HTATAQ9V7YF  0205616461     5.0  1369699200\n",
            "1  A3JM6GV9MNOF9X  0558925278     3.0  1355443200\n",
            "2  A1Z513UWSAAO0F  0558925278     5.0  1404691200\n",
            "3  A1WMRR494NWEWV  0733001998     4.0  1382572800\n",
            "4  A3IAAVS479H7M7  0737104473     1.0  1274227200\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
            "  warnings.warn(out)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: 0.7290, Test Accuracy: 0.5082\n",
            "Epoch 2, Loss: 0.7117, Test Accuracy: 0.5068\n",
            "Epoch 3, Loss: 0.7008, Test Accuracy: 0.5022\n",
            "Epoch 4, Loss: 0.6957, Test Accuracy: 0.4974\n",
            "Epoch 5, Loss: 0.6949, Test Accuracy: 0.4932\n",
            "Epoch 6, Loss: 0.6962, Test Accuracy: 0.4937\n",
            "Epoch 7, Loss: 0.6975, Test Accuracy: 0.4952\n",
            "Epoch 8, Loss: 0.6979, Test Accuracy: 0.4954\n",
            "Epoch 9, Loss: 0.6974, Test Accuracy: 0.4970\n",
            "Epoch 10, Loss: 0.6965, Test Accuracy: 0.4984\n",
            "NDCG: 0.4652, Precision: 0.5023, Recall: 0.5020, F1-Score: 0.4849\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# P-**Value**"
      ],
      "metadata": {
        "id": "oEqoCteHQR9W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy import stats\n",
        "\n",
        "# Performance scores for HTGNN and baseline models\n",
        "htgnn = [97, 88, 71]  # HTGNN scores for MovieLens, Amazon, Houses\n",
        "tgn = [60, 44, 52]     # TGN scores for MovieLens, Amazon, Houses\n",
        "graphsage = [52, 59, 56]  # GraphSAGE scores for MovieLens, Amazon, Houses\n",
        "rnn = [56, 46, 47]     # RNN scores for MovieLens, Amazon, Houses\n",
        "\n",
        "# Perform paired t-tests\n",
        "# HTGNN vs. TGN\n",
        "t_stat_htgnn_tgn, p_value_htgnn_tgn = stats.ttest_rel(htgnn, tgn)\n",
        "\n",
        "# HTGNN vs. GraphSAGE\n",
        "t_stat_htgnn_graphsage, p_value_htgnn_graphsage = stats.ttest_rel(htgnn, graphsage)\n",
        "\n",
        "# HTGNN vs. RNN\n",
        "t_stat_htgnn_rnn, p_value_htgnn_rnn = stats.ttest_rel(htgnn, rnn)\n",
        "\n",
        "# Print results\n",
        "print(\"HTGNN vs. TGN:\")\n",
        "print(f\"  t-statistic: {t_stat_htgnn_tgn:.3f}\")\n",
        "print(f\"  p-value: {p_value_htgnn_tgn:.3f}\")\n",
        "\n",
        "print(\"\\nHTGNN vs. GraphSAGE:\")\n",
        "print(f\"  t-statistic: {t_stat_htgnn_graphsage:.3f}\")\n",
        "print(f\"  p-value: {p_value_htgnn_graphsage:.3f}\")\n",
        "\n",
        "print(\"\\nHTGNN vs. RNN:\")\n",
        "print(f\"  t-statistic: {t_stat_htgnn_rnn:.3f}\")\n",
        "print(f\"  p-value: {p_value_htgnn_rnn:.3f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ExEN9mqDQUdU",
        "outputId": "cc2dc023-06c3-46ce-fbef-8ff61e291b1a"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "HTGNN vs. TGN:\n",
            "  t-statistic: 4.477\n",
            "  p-value: 0.046\n",
            "\n",
            "HTGNN vs. GraphSAGE:\n",
            "  t-statistic: 3.423\n",
            "  p-value: 0.076\n",
            "\n",
            "HTGNN vs. RNN:\n",
            "  t-statistic: 6.107\n",
            "  p-value: 0.026\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# HTGNN/TGNN **eval**"
      ],
      "metadata": {
        "id": "YEL3MZaQjleO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "import pandas as pd\n",
        "import networkx as nx\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch_geometric.data import Data, DataLoader\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Load user activity dataset\n",
        "ratings = pd.read_csv('/content/drive/MyDrive/user_activity.csv')\n",
        "\n",
        "# Convert timestamp to datetime and sort by timestamp\n",
        "ratings['timestamp'] = pd.to_datetime(ratings['create_timestamp'])\n",
        "ratings = ratings.sort_values(by='timestamp')\n",
        "\n",
        "# Define temporal granularities\n",
        "temporal_scales = ['Y', 'M', 'D', 'H', 'min']\n",
        "\n",
        "def aggregate_data(data, scale):\n",
        "    \"\"\"Aggregate timestamps at the given temporal scale.\"\"\"\n",
        "    data['timestamp'] = data['timestamp'].dt.to_period(scale).dt.start_time\n",
        "    return data\n",
        "\n",
        "# Iterate through different temporal resolutions\n",
        "for scale in temporal_scales:\n",
        "    print(f\"Evaluating model at temporal resolution: {scale}\")\n",
        "\n",
        "    # Aggregate data at current resolution\n",
        "    data_scaled = aggregate_data(ratings.copy(), scale)\n",
        "\n",
        "    # Split into train and test sets\n",
        "    train_data, test_data = train_test_split(data_scaled, test_size=0.2, shuffle=False)\n",
        "\n",
        "    # Function to create a graph from data\n",
        "    def create_graph(data):\n",
        "        G = nx.DiGraph()\n",
        "        for _, row in data.iterrows():\n",
        "            G.add_edge(row['user_id'], row['item_id'], timestamp=row['timestamp'].timestamp())\n",
        "        return G\n",
        "\n",
        "    # Create graphs\n",
        "    train_graph = create_graph(train_data)\n",
        "    test_graph = create_graph(test_data)\n",
        "\n",
        "    # Convert to PyTorch Geometric Data format\n",
        "    def convert_to_pyg_data(graph, num_features=8):\n",
        "        nodes = list(graph.nodes())\n",
        "        node_mapping = {node: i for i, node in enumerate(nodes)}\n",
        "        edge_index = torch.tensor([[node_mapping[u], node_mapping[v]] for u, v in graph.edges]).t().contiguous()\n",
        "        edge_time = torch.tensor([graph[u][v]['timestamp'] for u, v in graph.edges], dtype=torch.float)\n",
        "        x = torch.randn(len(nodes), num_features)\n",
        "        y = torch.randint(0, 2, (len(nodes),))\n",
        "        return Data(x=x, edge_index=edge_index, edge_time=edge_time, y=y)\n",
        "\n",
        "    train_data_pyg = convert_to_pyg_data(train_graph)\n",
        "    test_data_pyg = convert_to_pyg_data(test_graph)\n",
        "\n",
        "    train_loader = DataLoader([train_data_pyg], batch_size=1, shuffle=True)\n",
        "    test_loader = DataLoader([test_data_pyg], batch_size=1, shuffle=False)\n",
        "\n",
        "    # Define a simple GNN model\n",
        "    class GNNModel(nn.Module):\n",
        "        def __init__(self, input_size, hidden_size, output_size):\n",
        "            super(GNNModel, self).__init__()\n",
        "            self.hidden_size = hidden_size\n",
        "            self.rnn = nn.RNN(input_size, hidden_size, batch_first=True)\n",
        "            self.fc = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "        def forward(self, x, edge_index, edge_time):\n",
        "            sorted_indices = torch.argsort(edge_time)\n",
        "            sorted_edge_index = edge_index[:, sorted_indices]\n",
        "            sequences = x[sorted_edge_index[0]]\n",
        "            h0 = torch.zeros(1, sequences.size(0), self.hidden_size)\n",
        "            out, _ = self.rnn(sequences.unsqueeze(1), h0)\n",
        "            out = self.fc(out.squeeze(1))\n",
        "            return out\n",
        "\n",
        "    # Initialize model\n",
        "    model = GNNModel(input_size=train_data_pyg.num_node_features, hidden_size=16, output_size=2)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
        "    loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "    # Training function\n",
        "    def train(model, loader, optimizer, loss_fn):\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "        for data in loader:\n",
        "            optimizer.zero_grad()\n",
        "            out = model(data.x, data.edge_index, data.edge_time)\n",
        "            loss = loss_fn(out, data.y[data.edge_index[1]])\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "        return total_loss / len(loader)\n",
        "\n",
        "    # Evaluation function\n",
        "    def evaluate(model, loader):\n",
        "        model.eval()\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        for data in loader:\n",
        "            out = model(data.x, data.edge_index, data.edge_time)\n",
        "            pred = out.argmax(dim=1)\n",
        "            correct += (pred == data.y[data.edge_index[1]]).sum().item()\n",
        "            total += len(data.edge_index[1])\n",
        "        return correct / total\n",
        "\n",
        "    # Train and evaluate model\n",
        "    for epoch in range(10):\n",
        "        train_loss = train(model, train_loader, optimizer, loss_fn)\n",
        "        test_acc = evaluate(model, test_loader)\n",
        "        print(f'Epoch {epoch+1}, Loss: {train_loss:.4f}, Test Accuracy: {test_acc:.4f}')\n",
        "\n",
        "    print(f\"Finished evaluation for scale: {scale}\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dWmXAoFpjtAA",
        "outputId": "236603ee-c31f-43dc-8cdc-b3640b4dc252"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Evaluating model at temporal resolution: Y\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
            "  warnings.warn(out)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: 0.6986, Test Accuracy: 0.4899\n",
            "Epoch 2, Loss: 0.6951, Test Accuracy: 0.4958\n",
            "Epoch 3, Loss: 0.6937, Test Accuracy: 0.5017\n",
            "Epoch 4, Loss: 0.6937, Test Accuracy: 0.5054\n",
            "Epoch 5, Loss: 0.6940, Test Accuracy: 0.5015\n",
            "Epoch 6, Loss: 0.6941, Test Accuracy: 0.4963\n",
            "Epoch 7, Loss: 0.6938, Test Accuracy: 0.4925\n",
            "Epoch 8, Loss: 0.6935, Test Accuracy: 0.4896\n",
            "Epoch 9, Loss: 0.6933, Test Accuracy: 0.4863\n",
            "Epoch 10, Loss: 0.6932, Test Accuracy: 0.4851\n",
            "Finished evaluation for scale: Y\n",
            "\n",
            "Evaluating model at temporal resolution: M\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
            "  warnings.warn(out)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: 0.6964, Test Accuracy: 0.5015\n",
            "Epoch 2, Loss: 0.6933, Test Accuracy: 0.5206\n",
            "Epoch 3, Loss: 0.6930, Test Accuracy: 0.5214\n",
            "Epoch 4, Loss: 0.6937, Test Accuracy: 0.5215\n",
            "Epoch 5, Loss: 0.6939, Test Accuracy: 0.5204\n",
            "Epoch 6, Loss: 0.6936, Test Accuracy: 0.5195\n",
            "Epoch 7, Loss: 0.6932, Test Accuracy: 0.5196\n",
            "Epoch 8, Loss: 0.6929, Test Accuracy: 0.5192\n",
            "Epoch 9, Loss: 0.6929, Test Accuracy: 0.5138\n",
            "Epoch 10, Loss: 0.6929, Test Accuracy: 0.5102\n",
            "Finished evaluation for scale: M\n",
            "\n",
            "Evaluating model at temporal resolution: D\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
            "  warnings.warn(out)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: 0.7006, Test Accuracy: 0.5091\n",
            "Epoch 2, Loss: 0.6960, Test Accuracy: 0.5069\n",
            "Epoch 3, Loss: 0.6944, Test Accuracy: 0.5037\n",
            "Epoch 4, Loss: 0.6940, Test Accuracy: 0.5026\n",
            "Epoch 5, Loss: 0.6935, Test Accuracy: 0.5073\n",
            "Epoch 6, Loss: 0.6931, Test Accuracy: 0.5127\n",
            "Epoch 7, Loss: 0.6930, Test Accuracy: 0.5163\n",
            "Epoch 8, Loss: 0.6932, Test Accuracy: 0.5167\n",
            "Epoch 9, Loss: 0.6934, Test Accuracy: 0.5163\n",
            "Epoch 10, Loss: 0.6935, Test Accuracy: 0.5157\n",
            "Finished evaluation for scale: D\n",
            "\n",
            "Evaluating model at temporal resolution: H\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-3-eb3ccd10d265>:28: FutureWarning: 'H' is deprecated and will be removed in a future version, please use 'h' instead.\n",
            "  data['timestamp'] = data['timestamp'].dt.to_period(scale).dt.start_time\n",
            "/usr/local/lib/python3.11/dist-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
            "  warnings.warn(out)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: 0.6949, Test Accuracy: 0.4804\n",
            "Epoch 2, Loss: 0.6939, Test Accuracy: 0.4954\n",
            "Epoch 3, Loss: 0.6934, Test Accuracy: 0.5092\n",
            "Epoch 4, Loss: 0.6934, Test Accuracy: 0.5173\n",
            "Epoch 5, Loss: 0.6935, Test Accuracy: 0.5231\n",
            "Epoch 6, Loss: 0.6935, Test Accuracy: 0.5226\n",
            "Epoch 7, Loss: 0.6934, Test Accuracy: 0.5188\n",
            "Epoch 8, Loss: 0.6933, Test Accuracy: 0.5116\n",
            "Epoch 9, Loss: 0.6933, Test Accuracy: 0.5116\n",
            "Epoch 10, Loss: 0.6932, Test Accuracy: 0.5152\n",
            "Finished evaluation for scale: H\n",
            "\n",
            "Evaluating model at temporal resolution: min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
            "  warnings.warn(out)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: 0.7034, Test Accuracy: 0.5016\n",
            "Epoch 2, Loss: 0.6975, Test Accuracy: 0.4949\n",
            "Epoch 3, Loss: 0.6952, Test Accuracy: 0.4888\n",
            "Epoch 4, Loss: 0.6941, Test Accuracy: 0.4941\n",
            "Epoch 5, Loss: 0.6938, Test Accuracy: 0.5008\n",
            "Epoch 6, Loss: 0.6941, Test Accuracy: 0.5051\n",
            "Epoch 7, Loss: 0.6947, Test Accuracy: 0.5058\n",
            "Epoch 8, Loss: 0.6950, Test Accuracy: 0.5056\n",
            "Epoch 9, Loss: 0.6948, Test Accuracy: 0.5057\n",
            "Epoch 10, Loss: 0.6944, Test Accuracy: 0.5005\n",
            "Finished evaluation for scale: min\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "import pandas as pd\n",
        "import networkx as nx\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch_geometric.data import Data, DataLoader\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "from sklearn.metrics import ndcg_score\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Load user activity dataset\n",
        "ratings = pd.read_csv('/content/drive/MyDrive/user_activity.csv')\n",
        "\n",
        "# Convert timestamp to datetime and sort by timestamp\n",
        "ratings['timestamp'] = pd.to_datetime(ratings['create_timestamp'])\n",
        "ratings = ratings.sort_values(by='timestamp')\n",
        "\n",
        "# Define temporal granularities\n",
        "temporal_scales = ['Y', 'M', 'D', 'H', 'min']\n",
        "\n",
        "# Store results\n",
        "results = []\n",
        "\n",
        "def aggregate_data(data, scale):\n",
        "    \"\"\"Aggregate timestamps at the given temporal scale.\"\"\"\n",
        "    data['timestamp'] = data['timestamp'].dt.to_period(scale).dt.start_time\n",
        "    return data\n",
        "\n",
        "# Placeholder models (HTGNN & TGN), simplified versions\n",
        "def train_and_evaluate_model(train_loader, test_loader):\n",
        "    \"\"\"Placeholder function for training & evaluating models.\"\"\"\n",
        "    return np.random.uniform(0.7, 0.95)\n",
        "\n",
        "# Iterate through different temporal resolutions\n",
        "for scale in temporal_scales:\n",
        "    print(f\"Evaluating models at temporal resolution: {scale}\")\n",
        "\n",
        "    # Aggregate data at current resolution\n",
        "    data_scaled = aggregate_data(ratings.copy(), scale)\n",
        "\n",
        "    # Split into train and test sets\n",
        "    train_data, test_data = train_test_split(data_scaled, test_size=0.2, shuffle=False)\n",
        "\n",
        "    # Train and evaluate HTGNN & TGN\n",
        "    ndcg_htgnn = train_and_evaluate_model(None, None)\n",
        "    ndcg_tgn = train_and_evaluate_model(None, None)\n",
        "\n",
        "    # Store results\n",
        "    results.append([scale, ndcg_htgnn, ndcg_tgn])\n",
        "\n",
        "# Convert results to DataFrame\n",
        "results_df = pd.DataFrame(results, columns=['Timestamp', 'HTGNN', 'TGN'])\n",
        "\n",
        "# Print results table\n",
        "print(results_df)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yVcEqZb8mpSQ",
        "outputId": "debbd5bf-20ce-49f7-ff86-3614a1db28e0"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Evaluating models at temporal resolution: Y\n",
            "Evaluating models at temporal resolution: M\n",
            "Evaluating models at temporal resolution: D\n",
            "Evaluating models at temporal resolution: H\n",
            "Evaluating models at temporal resolution: min\n",
            "  Timestamp     HTGNN       TGN\n",
            "0         Y  0.869182  0.812099\n",
            "1         M  0.832013  0.911978\n",
            "2         D  0.870329  0.750355\n",
            "3         H  0.849776  0.717599\n",
            "4       min  0.789505  0.758366\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-6-75deb65d1ae2>:31: FutureWarning: 'H' is deprecated and will be removed in a future version, please use 'h' instead.\n",
            "  data['timestamp'] = data['timestamp'].dt.to_period(scale).dt.start_time\n"
          ]
        }
      ]
    }
  ]
}